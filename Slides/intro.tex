\documentclass[11pt,dvipsnames,usenames,times]{beamer}
%\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}
\usepackage{graphicx}
\setbeameroption{show notes}
\usepackage[round]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{newtxtext}
\usepackage{Sweave}
\DeclareGraphicsExtensions{.ps,.eps,.pdf,.jpg,.png}
\usefonttheme{professionalfonts}
%\usefonttheme[onlymath]{serif}
\usepackage[cmintegrals,cmbraces]{newtxmath}
\usetheme{default}
\usecolortheme{dove}
\usepackage{cancel}
\usepackage{tikz}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{color}
\usepackage{pgf} %portable graphics format
\usepackage[autobold]{statex2}
\mode<presentation>
{
  %\usetheme{Warsaw}
  % or ...
  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)

  \setbeamertemplate{navigation symbols}{}
  \usefonttheme[onlysmall]{structurebold}
  %\usefonttheme{structurebold}
}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
  \setbeamertemplate{navigation symbols}{}
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\newcommand*{\BART}{\mathrm{BART}\ }
%\newcommand*{\Wei}[2]{\mathrm{Wei}\wrap[()]{#1, #2}}
\newcommand*{\HBART}{\mathrm{HBART}\ }
\newcommand*{\corr}{\mathrm{corr}}
\newcommand*{\abs}{\mathrm{abs}}
\newcommand*{\DP}[2]{\mb{\mathrm{DP}}\wrap[()]{\mb{#1,\ #2}}}
%\newcommand*{\EV}[2]{\mb{\mathrm{ExtremeValue}}\wrap[()]{\mb{#1,\ #2}}}
%\newcommand*{\Wei}[2]{\mathrm{Wei}\wrap[()]{#1, #2}}
\newcommand*{\st}[1]{\mathrm{t}\wrap[()]{#1}}
\renewcommand*{\Dir}[1]{\mathrm{D}\wrap[()]{#1}}
\newcommand*{\nst}[3]{\mathrm{t}\wrap[()]{#1, #2, #3}}
\renewcommand*{\derivf}[2]{{\frac{\mathrm{d}}{\mathrm{d}#2}}\wrap{#1}}
\newcommand*{\derivfrac}[2]{{\frac{\partial#1}{\partial#2}}}
%\newcommand*{\derivfrac}[2]{{\frac{\mathrm{d}#1}{\mathrm{d}#2}}}
\newcommand*{\Nderivfrac}[2]{{\frac{\partial^{2}#1}{\partial^{2}#2}}}
%\newcommand*{\derivfrac2}[2]{{\frac{\mathrm{d}^2#1}{\mathrm{d}^2#2}}}
%\renewcommand*{\partialf}[2]{{\frac{\partial}{\partial #2}}\wrap{#1}}
\newcommand*{\Iff}{\;\mb{\mathrm{iff}}\;}
\newcommand*{\code}[1]{{\tt #1}}
\newcommand*{\pkg}[1]{{\tt #1}}
\newcommand*{\proglang}[1]{{\tt #1}}

%\newcommand*{\partialf}[2]{{\frac{\partial}{\partial #2}}\wrap{#1}}
%\DeclareRobustCommand*{\V}[2][]{\mb{\mathrm{V}}\ifthenelse{\equal{#1}{}}{}{_{\mb{#1}}} \wrap{\mb{#2}}}
\newcommand*{\partialf}[3][]{{\frac{\partial\ifthenelse{\equal{#1}{}}{}{^{\mb{#1}}}}{\partial #3\ifthenelse{\equal{#1}{}}{}{^{\mb{#1}}}}}\wrap{#2}}

\definecolor{color0}{rgb}{0,0,0}
\definecolor{color1}{rgb}{1,0,0}
\definecolor{color2}{rgb}{0,1,0}
\definecolor{color3}{rgb}{0,0,1}


\title{\textcolor{blue}{\normalsize 
An introduction to Bayesian Additive Regression Trees (BART)}}
\author{Rodney Sparapani\\
Associate Professor of Biostatistics\\
\textcolor{PineGreen}{\bf Medical College of Wisconsin}
%%Copyright (c) 2017-2022 Rodney Sparapani
}
\date{
September 15, 2025
}

\begin{document}

\titlepage
% \begin{quote}
% Funding for this research was provided, in part, by the\\
% Advancing Healthier Wisconsin Research \& Education Program
% under awards 9520277 and 9520364.
% \end{quote}
\boldmath
% 0. Intro

% \begin{frame}[fragile]\frametitle{Outline of Material}

% \begin{itemize}
% \item Part A: Introduction to BART
% \begin{itemize}
% \item Motivating Example: Growth Charts
% \item Bayesian Additive Regression Trees (BART)
% \item Heteroskedastic Bayesian Additive Regression Trees (HBART)
% \item The {\bf BART} package and other BART software
% \item The BART prior
% \item Friedman's partial dependence function
% \item Returning to growth chart example
% \item Posterior MCMC 
% \end{itemize}
% \item Part B: BART computational considerations 
% \begin{itemize}
% \item Installing the {\bf BART} R package
% \item The {\bf Rcpp} R package
% \item The {\bf BART} package and other BART software
% \item A brief overview of multi-processing/-threading
% \item Multi-threading with the {\bf BART} R package
% \item Missing imputation and multiple imputation
% \item Calling BART R functions and \texttt{predict}
% \item Creating a BART executable with C++ sans R
% \end{itemize}
% \item Conclusions
% \item Q \& A
% \end{itemize}
% \begin{comment}
% \item Selected BART Literature
% \item Current State of BART Software
% \item Binary Trees and BART
% \item The BART prior
% \item Theory of Ensembles
% \item BART and Nonparametric Theory
% \item Marginal Effects 
% \item Example
% \item Dichotomous Outcomes and Their Marginal Effects 
% \end{comment}

% \end{frame}

\begin{comment}
\begin{frame}
\frametitle{HW problem 10}

\begin{itemize}
\item For about 20 years, Froedtert/MCW have been accepting donations
of surgically removed tissue
\item These are held in a research tissue bank
\item Researchers in the Cardiovascular Center extract the vessels
\item At low flow (water pressure), the vessels are at rest
\item However, as flow is experimentally increased, the vessels dilate
\item Dilation is measured as a percentage of the maximum dilation
\item Occasionally, constriction occurs, i.e., dilation can be $<0$
\item But, values $>100$ are impossible: a data boundary
\item Many characteristics of the donors are collected such as 
gender, race/ethnicity, age, comorbid conditions, etc.
\item For example, how is age related to vessel dilation? 
\item The data is in the examples/vessels subdirectory: h2o\_human\_adipose.csv
\item With BART, create 3 figures for flow and age like slides 33-36
\end{itemize}

\end{frame}
\end{comment}

\begin{frame}%\frametitle{Motivating Example: Growth Charts}
\begin{center}
\scalebox{0.924}{\includegraphics{growth-height.eps}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Motivating Example: Growth Charts}
\begin{itemize}
\item The US Centers for Disease Control and Prevention (CDC) as well
  as the World Health Organization have developed growth charts for
  childhood development: height by age,\\ weight by age, body mass
  index by age and weight by height
\item Here we will focus on \textcolor{blue}{height}, $y_t$,\\ 
by \textcolor{red}{age} in months, $t=24, \dots, 215$ (2 to 17 years old)
%which is clearly non-linear
\item The CDC uses the LMS method via natural cubic splines \\
(Cole and Green 1992 {\it Statistics in Medicine})
\item Three parameters estimated by penalized maximum likelihood \\
the Box-Cox power transformation, $L_t$;\\ the mean, $M_t$; and
the coefficient of variation, $S_t$
\begin{align*}
z_t & = \left\{\begin{array}{cc} 
\frac{-1+(y_t/M_t)^{L_t}}{L_t S_t} & L_t \not= 0 \\
\frac{\log(y_t/M_t)}{S_t} &  L_t = 0 \end{array} 
\right\} ~\N{0}{1} 
\end{align*}
\item But, this only uses part of the data: just males or just females
\item Male/female 
trajectories are quite similar until about age 12
\item So what if we wanted to use all of the data? 
%\item Or include more information like weight or race/ethnicity?
% \item LMS is a flexible parametric estimation procedure but it is not a
%   model in the usual sense, i.e., it creates point-by-point estimates
%   that are simply smoothed together
% \item Cole TJ. The LMS method for
% constructing normalized growth
% standards. Eur J Clin Nutr 44:45–60.
% 1990.
% \item Cole TJ, Green PJ. Smoothing
% reference centile curves: The LMS
% method and penalized likelihood. Stat
% Med 11:1305–19. 1992.
%\item Let's model the data with BART an an alternative
% \item As an alternative, let's use BART\\ to model
% height as a function of age; gender; \\ 
% race/ethnicity (whites, blacks and Hispanics) and weight
% \item The CDC used three waves of the US National Health 
% and Nutrition Examination Survey (NHANES)
% \item For simplicity, just using one wave: data
% set available in the BART3 package as \texttt{bmx}
% (Body Measures Examination)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{What is Machine Learning Regression?}}
\boldmath

\begin{itemize}
\item The more appropriate term is ``Statistical Learning''\\ but 
``Machine Learning'' has caught on so we are stuck with it
\item Machine Learning Regression (MLR) is within the paradigm\\
 of Artificial (or Computational) Intelligence
\item MLR is extensible, but for the moment consider the general
  regression case of a continuous outcome with Normal errors
\begin{align*}
y_i & = \mu + \textcolor{blue}{f}(x_i) + \epsilon_i 
& \where \epsilon_i \iid \N{0}{\textcolor{black}{\sd^2}}
\end{align*}
\item %\textcolor{red}{\it Ideally} 
$\textcolor{blue}{f}$ is an
  unspecified %nonparametric
  function whose form is to be {\it learned} from the data and $x_i$
  is a vector of covariates for $i=1, \dots, N$
% \item $\textcolor{blue}{f}$ is a functional ensemble: 
% a complex model which is a simple summary of many simple models\\
% $\textcolor{blue}{f}\prior$BART is a sum of trees ensemble
\item {\it Ideally}, in a \textcolor{blue}{\it nonparametric} manner
  without resorting to \textcolor{red}{\it precarious restrictive
    assumptions}
% \item Least squares has been around for 200 years and we don't need
%   advanced computing for that beyond linear algebra: BLAS/LINPACK
%   have been around since the 1970s
% unknown ensembles\\
% $\textcolor{blue}{s}\prior$HBART is a product of trees ensemble\\
% Heteroskedastic BART (HBART)
\end{itemize}
\end{frame}
\begin{comment}
\item A common extension in MLR 
\begin{align*}
y_i & = \mu + \textcolor{blue}{f}(x_i) + \sd \textcolor{red}{s}(x_i)\epsilon_i 
& \where \epsilon_i \iid F_{\epsilon}
\end{align*}
\item And $\textcolor{blue}{f}$ and $\textcolor{red}{s}$ will both 
be {\it learned}, but how?
\item {\it Ideally} in a 
\textcolor{blue}{\it nonparametric} manner
without resorting to \textcolor{red}{\it precarious restrictive assumptions}
\end{comment}

\begin{frame}[fragile]\frametitle{\bf\textcolor{black}
{What is \textcolor{red}{Bayesian Additive Regression Trees}?}}

\begin{comment}
Chipman, George \&  \textcolor{Maroon}{McCulloch} 2010 {\it Annals of Applied Statistics}\\
%\textcolor{red}{Pratola}, \textcolor{Maroon}{McCulloch}, et al.\ 2014 {\it JCGS}\\
\textcolor{PineGreen}{Sparapani}, \textcolor{PineGreen}{Logan},
\textcolor{Maroon}{McCulloch} \& \textcolor{PineGreen}{Laud} 2016 {\it Statistics in Medicine}\\
%Linero 2018 {\it JASA}\\
\textcolor{red}{Pratola} 2016 {\it Bayesian Analysis}\\ 
%\textcolor{PineGreen}{Logan}, \textcolor{PineGreen}{Sparapani}, 
%\textcolor{Maroon}{McCulloch} \& \textcolor{PineGreen}{Laud} 2019 {\it SMMR}\\
%\textcolor{PineGreen}{Sparapani} et al.\ 2019 {\it J Am Heart Assoc}\\
%\textcolor{PineGreen}{Sparapani}, \textcolor{PineGreen}{Rein} et al.\ 2020 {\it Biostatistics}\\
%\textcolor{PineGreen}{Sparapani}, \textcolor{PineGreen}{Logan},
%\textcolor{Maroon}{McCulloch} \& \textcolor{PineGreen}{Laud} (in press) {\it SMMR}\\
\textcolor{red}{Pratola}, Chipman, George \& \textcolor{Maroon}{McCulloch} 
2019 {\it JCGS}\\
%George et al.\ Laud, Logan, McCulloch \& Sparapani (in press)
%{\it Advances in Econometrics }\\ 
\textcolor{PineGreen}{Sparapani}, Spanbauer \& \textcolor{Maroon}{McCulloch} 2021 {\it JSS}\\
R packages \url{https://github.com/rsparapa/bnptools}\\
\end{comment}
\begin{itemize}
\item a \textcolor{blue}{supervised MLR} with nice properties:
\textcolor{red}{automated learning} of the functional relationship 
and interactions without requiring covariate transformations
for continuous, binary, categorical and time-to-event outcomes\\
\item tree-based \textcolor{blue}{ensemble} predictive model 
\item \textcolor{red}{Bayesian nonparametric} method with\\
\textcolor{blue}{robust defaults} for the prior parameter settings
\item computationally efficient posterior inference via MCMC\\
estimates naturally computed from summaries of the posterior\\
along with the quantification of their uncertainty
\item seamless extension to \textcolor{red}{variable selection} 
in high dimensions
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{\bf\textcolor{black}
{\textcolor{red}{Selected BART references with URLs}}}
\begin{tabular}{l|l} \hline
Overview & \href{https://dx.doi.org/10.1214/09-AOAS285} 
{Chipman, George and McCulloch 2010} {\it AOAS} \\
& \textcolor{blue}{\href{https://doi.org/10.18637/jss.v097.i01}
{Sparapani, Spanbauer and McCulloch 2021} {\it JSS}} \\ \hline
%Survival Analysis & \href{https://doi.org/10.1093/bioinformatics/btq660}
%{Bonato, Baladandayuthapani et al.\ 2011} {\it Bioinform} \\
Survival Analysis & \href{https://doi.org/10.1002/sim.6893} 
{Sparapani, Logan et al.\ 2016} {\it Statistics in Medicine}\\
& \href{https://doi.org/10.1093/biostatistics/kxy028}
{Henderson, Louis et al.\ 2020} {\it Biostatistics} \\
& \href{https://doi.org/10.1093/biostatistics/kxy032}
{Sparapani, Rein et al.\ 2020} {\it Biostatistics}\\
& \href{https://doi.org/10.1177/0962280218822140}
{Sparapani, Logan et al.\ 2020} {\it SMMR}\\
& \href{https://doi.org/10.1214/21-BA1285}
{Linero, Basak et al.\ 2021} {\it Bayesian Analysis} \\ \hline
Big Data 
& \href{https://doi.org/10.1080/10618600.2013.841584}
{Pratola, Chipman et al.\ 2014} {\it JCGS}\\
(Big $N$) & \href{https://doi.org/10.1002/cjs.11343}
{Entezari, Craiu et al.\ 2017} {\it Canadian J of Stat} \\ \hline
%Variable Selection & \href{https://dx.doi.org/10.1214/14-AOAS755}
%{Bleich, Kapelner et al.\ 2014} {\it AOAS}\\
Variable Selection & \href{https://dx.doi.org/10.1080/01621459.2016.1264957}
{Linero 2018} {\it JASA}\\
(Big $P$)& \href{https://doi.org/10.1080/01621459.2021.1928514}
{Liu, Rockova 2021} {\it JASA} \\ \hline
Efficient MCMC & \href{https://dx.doi.org/10.1214/16-BA999}
{Pratola 2016} {\it Bayesian Analysis} \\ \hline
Nonparametric & \href{https://proceedings.mlr.press/v89/rockova19a.html}
{Rockova and Saha 2019} {\it PMLR} \\
Theory & \href{https://dx.doi.org/10.1214/19-AOS1879} 
{Rockova and van der Pas 2020} {\it AOS} \\ \hline
Heteroskedastic & \href{https://dx.doi.org/10.1080/10618600.2019.1677243}
{Pratola, Chipman et al.\ 2020} {\it JCGS} \\ \hline
Propensity Scores & \href{https://doi.org/10.1214/19-BA1195} {Hahn, Murray et al.\ 2020} {\it Bayesian Analysis} \\ \hline
Monotonic & \href{https://doi.org/10.1214/21-BA1259}
{Chipman, George et al.\ 2021} {\it Bayesian Analysis}
\end{tabular}
\end{frame}

\begin{comment}
\begin{frame}[fragile]\frametitle{BART software with a \texttt{predict} function}
\begin{tabular}{ll|ll|l}
      &          & \multicolumn{2}{|c|}{R packages} \\
Debut & Language & Stable (CRAN)        & Development    & Multi-threading \\ \hline 
2006  & C++      & \textcolor{blue}{\textbf{BayesTree}}  & None           & None \\ \hline
2013  & Java     & \textcolor{gray}{\textbf{bartMachine}} &              & Java \\ \hline
2014  & C++      & \textcolor{gray}{\textbf{dbarts}} &              & forking \\ \hline
2014  & C++      & \multicolumn{2}{l|}{MPI BART source code} & MPI \\ \hline
2017  & C++      & \textbf{BART} 2.9*        & \textbf{BART3}* & OpenMP/forking \\
2019  & C++      & \textcolor{red}{\textbf{rbart} 1.0*}       & \textbf{hbart}* & OpenMP \\
2019  & C++      & None                  & 
\textcolor{magenta}{\textbf{mxBART}*} & OpenMP/forking \\
2021  & C++      & None                  & 
\textcolor{blue}{\textbf{mBART}*} & OpenMP/forking \\
2021  & C++      & \textbf{nftbart} 1.5*     & \textbf{nftbart}*& OpenMP \\ \hline
  &      & \multicolumn{2}{l|}{*Descendents of MPI BART} &  \\ \hline
\multicolumn{5}{l}{Development on \url{github.com} by users \texttt{rsparapa} (me),} \\
\multicolumn{5}{l}{\textcolor{magenta}{\texttt{cspanbauer} (Charley Spanbauer)} and \textcolor{blue}{\texttt{remcc} (Rob McCulloch)}}\\
\multicolumn{5}{l}{Special thanks to \textcolor{blue}{Rob} (\textbf{BART}),
\textcolor{red}{Matt Pratola for \textbf{rbart}}} \\
\multicolumn{5}{l}{Hugh Chipman, Robert Gramacy, the R Core team, } \\
\multicolumn{5}{l}{the Rcpp Core team and so many others in the FOSS community!}
\end{tabular}
\end{frame}

\begin{frame}[fragile]\frametitle{BART software features: descendents of MPI BART}

\begin{tabular}{l|lll|l}
Stable         & \textbf{BART} & \textbf{nftbart} & \textcolor{red}{\textbf{rbart}} \\ 
Development    & \textbf{BART3} & \textbf{nftbart} &  \textbf{hbart} & 
\textcolor{blue}{\textbf{mBART}}  \\ \hline
\texttt{github.com} user & \multicolumn{3}{c|}{\texttt{rsparapa}} & 
\textcolor{blue}{\texttt{remcc}} \\ \hline
\texttt{predict} function & Yes & Yes & Yes & \textbf{BART}  \\
heteroskedastic   & No  &  \textcolor{red}{Yes} &  \textcolor{red}{Yes} & No \\
monotonic         & No  & No  & No  & \textcolor{blue}{Yes} \\
continuous        & \textcolor{blue}{Yes} & {Yes} & 
{Yes} & \textcolor{blue}{Yes} \\
binary/categorical& {Yes} & No  & No  & No \\ \hline
right censoring   & Yes & Yes & No  & No \\
left  censoring   & No  & Yes & No  & No \\
competing risks   & Yes & No  & No  & No \\
recurrent events  & Yes & No  & No  & No \\ \hline
sparse prior      & \textcolor{magenta}{Yes} & No  & No  & No \\
marginal effects  & \textbf{BART3} & Yes & No & No \\
missing imputation& Yes & Yes & No  & No \\
advanced tree proposals & No  & \textcolor{red}{Yes} & \textcolor{red}{Yes} & No\\
nonparametric error&No  & Yes & No  & No \\
C++ header-only   &\textbf{BART3} & No & \textbf{hbart} & No 
\end{tabular}

\end{frame}
\end{comment}

\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}

Chipman, George \&  \textcolor{Maroon}{McCulloch} 2010 {\it Annals of Applied Stat}\\
%(ChipGeor10): BART package on CRAN, BART3 on my github\\ 
%\vspace*{0.2in}
%${ y_i={\mu}+g(x_i; \mathcal{T}_1, \mathcal{M}_1) + g(x_i; \mathcal{T}_2, \mathcal{M}_2) + \dots + g(x_i; \mathcal{T}_H, \mathcal{M}_H) + \epsilon_i}$\\

% \hspace*{0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}
% \hspace*{-0.3in}
% \reflectbox{\includegraphics[scale=0.05]{single-tree.png}}
% \hspace*{0.2in}${\cdots}$%\hspace*{-0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}
% \vspace*{0.15in}

%where $H \in \{50, 100, 200\}$ and $\epsilon_i \iid \N{0}{\sd^2}$
%$\epsilon_i|(f, \sd) \iid \N{0}{\sd^2}$
\begin{align*}
y_i&\ \ =\ \  \textcolor{red}{f(x_i)} + \epsilon_i &  \epsilon_i & \iid \N{0}{w_i^2 \sd^2}\\
%y_i&\ \ =\ \ {\mu}+ \textcolor{red}{f(x_i)} + \epsilon_i &  \epsilon_i & \iid \N{0}{w_i^2 \sd^2}\\
\textcolor{red}{f} &\ \prior\ \textcolor{red}{\BART}(\textcolor{red}{H,\mu,} \kappa, \tau, \alpha, \beta) \\
%\textcolor{red}{f} &\ \prior\ \textcolor{red}{\BART}(\alpha, \beta, H, \kappa, \mu, \tau) \\
\textcolor{red}{f(x_i)} &\ \ \equiv\ \ \mu+\textcolor{red}{\sum_{h=1}^H} \textcolor{blue}{g(x_i; \mathcal{T}_h, \textcolor{red}{\mathcal{M}_h})} & 
%\textcolor{red}{f(x_i)} &\ \ \equiv\ \ \textcolor{red}{\sum_{h=1}^H} \textcolor{blue}{g(x_i; \mathcal{T}_h, \textcolor{red}{\mathcal{M}_h})} & 
H & \in \{50, \textcolor{red}{200}, 500\} \\ %$ and $\epsilon_i \iid \N{0}{\sd^2}$
%\textcolor{red}{f} & \prior \textcolor{red}{\BART}(H, \kappa=2, \mu=\bar{y},  \dots) &
%\textcolor{blue}{\mu_{hl}} & \prior \N{0}{\sd_{\mu}^2} \where \sd_{\mu}= \frac{0.5\, \mathrm{range}(y)}{\kappa \sqrt{H}} 
\textcolor{red}{\mu_{hl}|}\textcolor{blue}{\mathcal{T}_h} & \prior \N{0}{\frac{\tau^2}{4H\kappa^2 }} 
\textcolor{blue}{\mbox{\ leaves of\ }{\mathcal{T}}_h} \\
%\mbox{\ \textcolor{red}{leaves}} \\
&\ \ \in\ \ \textcolor{red}{\mathcal{M}_h} \\
{\sd^2}& \prior \lambda\nu\IC{\nu}  \\ 
& \prior \IG{\nu/2}{\lambda\nu/2}  & \E{\sd^2} & = \lambda\nu/(\nu-2) \\ 
\end{align*}
\end{frame}

\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}
\textcolor{PineGreen}{Logan}, \textcolor{PineGreen}{Sparapani}, 
\textcolor{Maroon}{McCulloch} \& \textcolor{PineGreen}{Laud} 2020 {\it SMMR}\\
\begin{center}
\scalebox{1.3}{\includegraphics{figure1-upper.pdf}}\\
\vspace{-15mm}
\scalebox{1.3}{\includegraphics{figure1-lower.pdf}}
%\scalebox{1.25}{\includegraphics{figure1uw.pdf}}
\end{center}
\end{frame}

% \begin{frame}
% \boldmath
% \frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}
% \begin{center}
% \includegraphics[scale=0.485]{figure1uw.pdf}
% \end{center}
% \end{frame}

\begin{comment}
\begin{frame}
\frametitle{\bf\textcolor{blue}{Heteroskedastic BART (HBART)}}

\textcolor{red}{Pratola}, Chipman, George \& \textcolor{Maroon}{McCulloch} 
2020 {\it JCGS}\\
%2017 {\it arXiv preprint}\\

\begin{align*}
%y_i&\ \ =\ \ \mu+ {f(x_i)} 
y_i&\ \ =\ \ {f(x_i)} 
+ \textcolor{red}{s(x_i)}\epsilon_i & \epsilon_i & \iid \N{0}{w_i^2 \sd^2}\\
%{f} &\ \prior\ {\BART}(\alpha, \beta, H, \kappa, \mu, \tau) \\
{f} &\ \prior\ {\BART}( {H,\mu,} \kappa, \tau, \alpha, \beta) \\
%\where \widetilde{H} \in \{10, 20, 40\} \\
\textcolor{red}{s^2} &\ \prior\ \textcolor{red}{\HBART}(
\widetilde{H}, \tilde{\lambda}, \tilde{\nu}, \tilde{\alpha}, \tilde{\beta})
%\tilde{\alpha}, \tilde{\beta}, \widetilde{H}, \tilde{\lambda}, \tilde{\nu})
 \\
\textcolor{red}{s^2(x_i)} &\ \ \equiv\ \ \textcolor{red}{ \prod_{{h}=1}^{\widetilde{H}} } \textcolor{blue}{g(x_i; \widetilde{\mathcal{T}}_{{h}}, \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}})}  & \widetilde{H} & \approx H/5 \\
%\textcolor{red}{s^2(x_i)} &\ \ \equiv\ \ \textcolor{red}{ \prod_{{h}=1}^{\widetilde{H}} } \textcolor{blue}{g(x_i; \widetilde{\mathcal{T}}_{{h}}, \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}})}  & \widetilde{H} & \approx H/5 \\
\textcolor{red}{\sd^2_{{h}l}}|\textcolor{blue}{\widetilde{\mathcal{T}}_h}& 
\prior \lambda\nu\IC{\nu} 
\textcolor{blue}{\mbox{\ leaves of\ }\widetilde{\mathcal{T}}_h} 
& \lambda & = \tilde{\lambda}^{1/\widetilde{H}}\\
& \prior \IG{\nu/2}{\lambda\nu/2}  & \E{\textcolor{red}{\sd^2_{{h}l}}} & = \lambda\nu/(\nu-2) \\
%\\ %& \lambda & = \tilde{\lambda}^{1/\widetilde{H}} \\
&\ \ \in\ \ \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}}
& \nu & = 2 \wrap{1-\wrap[()]{1-\frac{2}{\tilde{\nu}}}^{1/\widetilde{H}}}^{-1} \\
%& \nu & = 2 \wrap{1-\wrap[()]{1-\frac{2}{\tilde{\nu}}}^{1/\widetilde{H}}}^{-1}
\end{align*}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{\bf\textcolor{blue}{BART, ensembles and prediction error}}
\boldmath

\begin{itemize}
\item $\textcolor{blue}{\mbox{mean squared error}= \mbox{bias}^2 + \mbox{variance}}$
\vspace*{0.1in}
\item There is a trade-off between the bias and variance
\vspace*{0.1in}
\item Consider the spectrum of trade-offs\\
\vspace*{0.1in}
Linear regression is on the high bias/low variance end\\
\vspace*{0.1in}
Single-tree regression is on the low bias/high variance end\\
\vspace*{0.1in}
\item Ensembles are in the middle:
medium bias/medium variance
\vspace*{0.1in}
\item \textcolor{red}{BART is in the class of ensemble models which
    both theoretically, and in practice, have excellent
out-of-sample predictive performance}
\end{itemize}

Krogh \& Solich 1997 {\it Physical Review E}\\
Baldi \& Brunak 2001 ``Bioinformatics: machine learning approach''\\
Kuhn \& Johnson 2013 ``Applied Predictive Modeling''
\end{frame}

\begin{frame}[fragile]
\frametitle{Binary trees and Bayesian Additive Regression Trees}

\begin{itemize}
\item 
BART relies on an ensemble of $H$ binary trees 
%which are a type of a directed acyclic graph
\item We exploit the wooden tree metaphor to its
fullest except binary trees grow downward by tradition
\item Each of these trees grows down starting out as
a root node
\item The root node is generally a branch decision rule, but
it doesn't have to be; occasionally, there are trees in the ensemble
which are only a root terminal node consisting of a single leaf output
value
\item If the root is a branch decision rule, then it spawns a left
and a right node which each can be either a branch decision rule or a
terminal leaf value and so on
\item In binary tree, $\mathcal{T}$, there
are $C$ nodes which are made of $B$ branches and $L$ leaves: $C=B+L$
\item 
There is an algebraic relationship between the number of branches and
leaves which we express as $B= L-1$.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART R package}
\begin{itemize}
\item to facilitate the {\tt predict} function,
BART fits can be stored as R objects to be reloaded later
\item the ensemble of trees is encoded in an ASCII string
which is returned in the {\tt treedraws\$trees} 
 list item%
\item This string can be read by {R}
\item Encoded with C/C++ indexing starting with 0 is used\\
rather than R object indexing starting with 1
\item Since the {\tt predict} function calls C/C++ code for speed
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART R package and trees}
Sparapani, Spanbauer and McCulloch 2021\\ 
{\it Journal of Statistical Software}
\begin{Sinput}
R> write(post$treedraws$trees, "trees.txt")
R> tc <- textConnection(post$treedraws$tree)
R> trees <- read.table(file=tc, fill=TRUE, row.names=NULL, header=FALSE,
+    col.names=c("node", "var", "cut", "leaf"))
R> close(tc)
R> head(trees)
\end{Sinput}
\begin{minipage}{6cm}
\begin{Soutput}
  node var cut         leaf
1 1000 200   1           NA
2    3  NA  NA           NA
3    1   0  66 -0.001032108
4    2   0   0  0.004806880
5    3   0   0  0.035709372
6    3  NA  NA           NA
\end{Soutput}
\end{minipage}
\begin{minipage}{3cm}
\usetikzlibrary{shadows}
 \begin{tikzpicture}
 [level distance=20mm,sibling distance=25mm,
   int/.style={fill=white,draw=black,drop shadow,circle,anchor=north},
   ter/.style={fill=white,rectangle,draw=black,drop shadow}]
\node[int]  {$x_1$} [grow=down]
child {node[ter] {$0.005$}
edge from parent node [left,pos=0.3] {$\le c_{1,67}$}}
child {node[ter] {$0.036$}
edge from parent node [right,pos=0.3] {$> c_{1,67}$}};
 % \node[int]  {$x_1$} [grow=up]
 % child {node[ter] {$0.036$}
 % edge from parent node [right,pos=0.3] {$> c_{1,67}$}}
 % child {node[ter] {$0.005$}
 % edge from parent node [left,pos=0.3] {$\le c_{1,67}$}};
 \end{tikzpicture} 
\end{minipage} 
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART R package and trees}
\begin{itemize}
\item
The {\tt treedraws\$trees} string is encoded as follows
\item The
first line is an exception which has the number of MCMC samples, $M$,
in the field {\tt node}; the number of trees, $H$, in the field
{\tt var}; and the number of variables, $P$, in the field {\tt cut}
\item 
For the rest of the file, the field {\tt node} is used for the number
of nodes in the tree when all other fields are {\tt NA}; or for a
specific node when the other fields are present
\item The nodes are
numbered in relation to the tree's depth level, 
$\textcolor{blue}{d(n)}=\lfloor \log_2 n \rfloor$ or
%$\textcolor{blue}{t(n)}=\lfloor \log_2 n \rfloor$ or
{\tt floor(log2(node))}
\end{itemize}
\begin{center}
\begin{tabular}{r|ccccccc} \hline
Depth & \\ \hline
0 &   &   &   & 1 &   &   &   \\ %\hline
1 &   & 2 &   &   &   & 3 &   \\
2 & 4 &   & 5 &   & 6 &   & 7 \\
$\vdots$ & \\
%$t$ & \multicolumn{3}{c}{$2^t$} & $\dots$ & \multicolumn{3}{c}{$2^{t+1}\!-\!1$} \\ \hline
$d$ & \multicolumn{3}{c}{$2^d$} & $\dots$ & \multicolumn{3}{c}{$2^{d+1}\!-\!1$} \\ \hline
% $t$ & \multicolumn{3}{c}{$2^t$} & $\dots$ &
% \multicolumn{3}{c}{$2^{t+1}\!-\!1$} \\ 
% $\vdots$ & \\
% 2 & 4 &   & 5 &   & 6 &   & 7 \\
% 1 &   & 2 &   &   &   & 3 &   \\
% 0 &   &   &   & 1 &   &   &   \\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART R package and trees}
\begin{itemize}
\item
The {\tt var} field is the variable in the branch decision rule which
is encoded $0, \dots, P-1$ as a {C}/{C++} 
index (rather than {R})
\item  Similarly, the {\tt cut}
field is the cut-point of the variable in the branch decision rule
which is encoded $0, \dots, c_j-1$\\ for variable $j$
\item 
cut-points are returned in the {\tt treedraws\$cutpoints} list item
\item 
The terminal leaf output value is contained in the field {\tt leaf}
\item 
It is not immediately obvious which nodes are branches vs.\ leaves
since, at first, it would appear that the {\tt leaf} field is given
for both branches and leaves
\item Confusingly: leaves are always associated with
{\tt var=cut=0}
\item however, note that this is also a valid
branch variable/cut-point since these are {C}/{C++}
indices
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART R package and trees}
\begin{itemize}
\item
% The key insight is that the 
% first $B$ rows of each node are branches and the rest are leaves.
The key to proper discrimination between branches and leaves is via the
algebraic relationship between a branch, $n$, at tree depth $d(n)$
leading to its left, $l=2n$, and right, $r=2n+1$, nodes at depth
$d(n)+1$
\item for each node, besides root,
%$l=2^{t+1}+2k$ and $r=2^{t+1}+2k+1$, i.e., for each node, besides root,
you can determine from which branch it arose and those nodes that are
not a branch (since they have no leaves) are necessarily leaves
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item
The BART prior
specifies a flexible class of unknown functions, $f$, from which we
can gather randomly generated fits to the given data via the
posterior
\item Here we define $f$ as returning a scalar value: for 
a multivariate BART example, see Um, Linero, et al.\ 2023 {\it Stat in Med}
\item Let 
function $g(\bm{x}; \mathcal{T}, \mathcal{M})$ assign a value based on
the input $\bm{x}$
\item The binary tree $\mathcal{T}$ is represented by 
a collection of $C$ four-tuples
 $(n, \psi_n; j, k)$: $n$ for node number;\\ 
$\psi_n=1$ for a branch and 0 for a leaf;\\ 
and, if a leaf, $j$ for covariate $x_j$ 
with $k$ for the cut-point $c_{jk}$
\item The collection of branches is denoted by
 $\mathcal{B}=\{ n : \psi_n=1 \}$
\item The branch decision rules are of the form
$x_j\le c_{jk}$ which means branch left and $x_j>c_{jk}$, branch
right; or terminal leaves where it stops. 
\item  $\mathcal{M}$ represents
leaves and is a set of ordered pairs, $(n, \mu_n)$:
$n \in \mathcal{L}$ where $\mathcal{L}$ is the set of leaves\\
($\mathcal{L}$ is the complement of $\mathcal{B}$) and $\mu_n$ for the
outcome value
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item
The function, $f(\bm{x})$, is the following sum:
\begin{align*}
%\label{BARTfunction}
f(\bm{x})=\mu+\sum_{h=1}^H g(\bm{x}; \mathcal{T}_h, \mathcal{M}_h)
\end{align*} 
where $H$ is ``large'', let's say, 50, {\bf 200} or 500
\item For a continuous outcome, $y_i$, we have the following BART regression
on the vector of covariates, $\bm{x}_i$:
\begin{align*}
y_i=f(\bm{x}_i)+\epsilon_i \where \epsilon_i \iid \N{0}{ w_i^2
  \sd^2}
\end{align*}
with $i$ indexing subjects $i=1, \dots, N$
\item The BART priors for the unknown random
function, $f$,\\ and the error variance, $\sd^2$, are 
as follows
\begin{align*}
f &\prior\BART(H, \mu, \kappa, \tau, \alpha, \beta) &
%f &\prior\BART(H, \mu, \tau, \kappa, \alpha, \beta) &
\sd^2 & \prior \nu \lambda \IC{\nu} 
%& \prior \IG{\nu/2}{\lambda\nu/2}  & \E{\sd^2} & = \lambda\nu/(\nu-2) 
%(f,\sd^2)\prior\mathrm{BART}(H, \mu, \tau, k, \alpha, \beta; \nu, \lambda, q)
\end{align*}
where $H$ is the number of trees, $\mu$ is a known constant which
centers ${y}$ and the rest of the parameters will be explained later
in this section (for brevity, we often use
$f \prior\mathrm{BART}$)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item The $w_i$ are known standard
deviation weight multiples which you can supply with the argument
\code{w} that is only available for continuous outcomes, hence, the
weighted BART name; the unit weight vector is the default
\item The
centering parameter, $\mu$, can be specified via the \code{fmean}
argument where the default is taken to be $\b{y}$
\item $x_i$: matrices or data frames can be supplied 
\item
unlike matrices, data frames can contain categorical factors
 when \code{x.train} is a data frame
\item Factors with multiple levels are transformed into dummy
variables with each level as their own binary
indicator; %while each continuous variable is a group all by itself
factors with only two levels are a binary indicator with a
single dummy variable
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item BART is a Bayesian nonparametric prior
\item we represent the BART prior in terms
of the collection of all trees, $\mathcal{T}$; collection of all leaves,
$\mathcal{M}$; and the error variance, $\sd^2$, as: 
 $\wrap{\mathcal{T}, \mathcal{M}, \sd^2}=
\wrap{\sd^2}\wrap{\mathcal{T}, \mathcal{M}}=
\wrap{\sd^2}\wrap{\mathcal{T}}\wrap{\mathcal{M}|\mathcal{T}}$
\item the individual trees themselves are independent:
$\wrap{\mathcal{T}, \mathcal{M}}=\prod_h
\wrap{\mathcal{T}_h}\wrap{\mathcal{M}_h|\mathcal{T}_h}$
where $\wrap{\mathcal{T}_h}$ is the prior for the $h$th tree and
$\wrap{\mathcal{M}_h|\mathcal{T}_h}$ is the collection of leaves for
the $h$th tree
\item the collection of leaves for the
$h$th tree are independent:
$\wrap{\mathcal{M}_h|\mathcal{T}_h}=
\prod_n\wrap{\mu_{hn}|\mathcal{T}_h}$
where $n$ indexes the leaf nodes
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item 
The tree prior: $\wrap{\mathcal{T}_h}$.  There are three prior
components of $\mathcal{T}_h$ which govern whether the tree branches
grow or are pruned
\item The first tree prior regularizes the probability of a branch at
  leaf node $n$ in tree depth
  $\textcolor{blue}{d(n)}=\lfloor\log_2 n\rfloor$ as follows.
\begin{align}\label{regularity}
\psi_n \prior 
\B{\textcolor{red}{p(\textcolor{blue}{d(n)})}} 
\where \textcolor{red}{p(\textcolor{blue}{d(n)})}
=\alpha (\textcolor{blue}{d(n)}+1)^{-\beta}
%\P{\psi_n=1}=\alpha (t(n)+1)^{-\beta}
\end{align}
$\psi_n=1$ represents
a branch while $\psi_n=0$ is a leaf\\ 
$0<\alpha<1$ and $\beta\ge 0$ 
\item You
can specify these prior parameters with arguments, but the following
defaults are recommended: $\alpha$ is set by the parameter
\code{base=0.95} and $\beta$ by \code{power=2}
%; for a detailed discussion of these parameter settings, see {ChipGeor98}
% \item Note
% that this prior penalizes branch growth, i.e., in prior probability,
% the default number of branches will likely be 1 or 2
\item The expected number of
branches~(leaves) is 1~(2) with probability
$\P{\psi_{1}=1, \psi_{2}=\psi_{3}=0}=p(0)q(1)^2 \approx 0.55$
\item Or 2~(3) with 
$2\P{\psi_{1}=\psi_{2}=1, \psi_{3}=\psi_{4}=\psi_{5}=0}=
2p(0)p(1)q(1)q(2)^2 \approx 0.27$
(doubled due to symmetry)
\item Trees with only 1 or 2 branches (2 or 3 leaves) would
dominate with a probability of about $0.82$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{BART and Bayesian nonparametric theory}}
\boldmath

\begin{itemize}
\item frequentist theoretical justification for BART's performance:\\
\textcolor{red}{asymptotically consistent} with a 
\textcolor{blue}{near optimal learning rate}
\vspace*{0.1in}
\item 
the BART posterior distribution concentrates around the
truth at a \textcolor{red}{near optimal minimax rate}\\
\vspace*{0.1in}
\item 
the default BART Branching penalty is \textcolor{red}{near optimal}:\\
 $\psi_n \prior \B{\alpha (1+\textcolor{blue}{d(n)})^{-\beta}}
\where \textcolor{blue}{d(n)}=0, \dots$ 
%  $\P{\mbox{Branch}|\textcolor{blue}{\mbox{tier}}}
% = \alpha (1+\textcolor{blue}{\mbox{tier}})^{-\beta}$ \\
% \vspace*{0.1in}
\item 
the \textcolor{red}{optimal} BART Branching penalty is now shown to be:\\
 $\psi_n \prior \B{\gamma^{\textcolor{blue}{d(n)}}} \where 0<\gamma<0.5$ 
\end{itemize}
\begin{tabular}{lcccc}
Branches (Leaves) & 0 (1) & 1 (2) & 2 (3)  & 3+ (4+)  \\
Prior probability& 0.00 & $(1-\gamma)^2$ & $2\gamma(1-\gamma)(1-\gamma^2)^2$ &
$\dots$ \\ 
$\gamma=0.25$    & 0.00 & 0.56 & 0.33 & 0.11 \\
$\alpha=0.95, \beta=2$    & 0.05 & 0.55 & 0.27 & 0.13 \\
\end{tabular}\\
\vspace*{0.1in}
Chipman, George \& McCulloch 1998 {\it JASA}\\
Rockova \& Saha 2018 {\it PMLR}\\
Rockova \& van der Pas 2020 {\it Annals of Statistics}\\
\end{frame}

\begin{comment}
\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item Alternatively, for variable selection, you can specify a Dirichlet
prior which is more appropriate if the number of covariates is large
\item we can represent the probability via the
sparse Dirichlet prior as
$\wrap{s_1, \dots, s_P}|\theta \prior \Dir{\theta/P, \dots, \theta/P}$ which
is specified by the argument \code{sparse=TRUE} while the default is
\code{sparse=FALSE} for uniform $s_j=P^{-1}$
\item The prior parameter
$\theta$ can be fixed or random: supplying a positive number will specify
$\theta$ fixed at that value while the default \code{theta=0} is random
and its value will be learned from the data
\item The random $\theta$ prior
is induced via $\theta/(\theta+\rho) \prior \Bet{a}{b}$ where the
parameter $\rho$ can be specified by the argument \code{rho} (which
defaults to \code{NULL} representing the value $P$; provide a value to
over-ride), the parameter $b$ defaults to 1 (which can be over-ridden
by the argument \code{b}) and the parameter $a$ defaults to 0.5 (which
can be over-ridden by the argument \code{a})
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item The distribution of
\code{theta} controls the sparsity of the model: \code{a=0.5} induces
a sparse posture while \code{a=1} is not sparse and similar to the
uniform prior with probability $s_j=P^{-1}$
\item 
If additional sparsity is desired,
then you can set the argument \code{rho} to a value smaller than $P$
\item 
Here, we take the opportunity to provide some insight into how and why
the sparse prior works as desired
\item The key to understanding the
inducement of sparsity is the distribution of the arguments to
the Dirichlet prior: $\theta/P$
\item it can be
shown that $\theta/P ~ F(a, b, \rho/P)$ where $F(.)$ is the Beta Prime
distribution scaled by $\rho/P$ 
\item  The non-sparse
setting is $(a, b, \rho/P)=(1, 1, 1)$
\item As you can see in the next figure, 
sparsity is increased by reducing $\rho$, reducing
$a$ and moreso by reducing both
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior\\
The distribution of $\theta/P$ and the sparse Dirichlet prior}
\begin{center}
\includegraphics[scale=0.4]{sparse-beta-prime.pdf}
\end{center}
\end{frame}
\end{comment}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item 
The leaf prior: $\wrap{\mu_{hn}|\mathcal{T}_h}$
\item Given a tree, $\mathcal{T}_h$, there is
a prior on its leaf values, $\mu_{hn}|\mathcal{T}_h$ and we denote the
collection of all leaves in $\mathcal{T}_h$ by
$\mathcal{M}_h=\{(n, \mu_{hn}): n \in \mathcal{L}_h \}$
\item
Suppose that $y \in [y_{\min}, y_{\max}]$ where\\ 
$y_{\min}$ and $y_{\max}$ might be elicited as $0.025$ and $0.975$ quantiles\\
otherwise, the observed min and max are used (the default)
\item Denote 
%$\wrap{\mu_{1(i)}, \dots, \mu_{H(i)}}$ as the leaf output values from each 
$\wrap{\tilde{\mu}_{1}, \dots, \tilde{\mu}_{H}}$ as the leaf output values from each 
tree corresponding to the vector of covariates, $\bm{x}$ %$\bm{x}_i$
\item 
If $\tilde{\mu}_{h}|\mathcal{T}_h \iid \N{0}{\sd_{\mu}^2}$, then the model 
estimate is 
$\hat{y}=\E{y|\bm{x}}=\mu+\sum_h\tilde{\mu}_{h}$ where
$\hat{y} ~ \N{\mu}{H \sd_{\mu}^2}$
% If $\mu_{h(i)}|\mathcal{T}_h \iid \N{0}{\sd_{\mu}^2}$, then the model 
% estimate for subject~$i$ is 
% $\mu_i=\E{y_i|\bm{x}_i}=\mu+\sum_h\mu_{h(i)}$ where
% $\mu_i ~ \N{\mu}{H \sd_{\mu}^2}$
\item Solve for $\sd_{\mu}$: %which is the solution to the equations
$y_{\min}=\mu-\kappa\sqrt{H}\sd_{\mu}$
and $y_{\max}=\mu+\kappa\sqrt{H}\sd_{\mu}$\\
$\sd_{\mu}=\frac{y_{\max}-y_{\min}}{2 \kappa \sqrt{H}}$
\item Therefore, we arrive at
$\mu_{hn} \prior \N{0}{{\frac{\tau^2}{4H\kappa^2}}} \where
\tau={y_{\max}-y_{\min}}$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item The parameter $\kappa$ calibrates this prior as follows
\item 
The default value, $\kappa=2$, corresponds to
$\hat{y}$ falling within the extrema with approximately 0.95
probability
\item  Alternative choices of
$\kappa$ can be supplied via the \code{k} argument
\item We have found that
values of $\kappa \in [1, 3]$ generally yield good results
\item Note that
$\kappa$ is a potential candidate parameter for choice via
cross-validation
%\item However, this could lead to over-fitting
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
%\item Other important arguments for the BART prior
\item We fix the number of
trees at $H$ which corresponds to the argument \code{ntree}
\item The
default number of trees is 200 for continuous outcomes; but for
computational convenience, 50 is also a reasonable choice which is the
default for all other outcomes
\item cross-validation could be considered
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item 
The number of cut-points is provided by the argument \code{numcut} and
the default is 100
\item The default number of cut-points is achieved for
continuous covariates
\item For continuous covariates, the cut-points are
uniformly distributed by default, or generated via uniform quantiles
if the argument \code{usequants=TRUE} is provided
\item By default,
discrete covariates which have fewer than 100 values will necessarily
have fewer cut-points
\item  However, if you want a single discrete
covariate to be represented by a group of binary dummy variables, one
for each category, then pass the variable as a factor within a data
frame
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{The BART prior}
\begin{itemize}
\item Next, there is
a prior dictating the choice of a splitting variable $j$ conditional on a
branch event $\psi_n$ which defaults to uniform probability $s_j=P^{-1}$ where
$P$ is the number of covariates 
\item Given a branch event, $\psi_n$, and a
variable chosen, $x_j$, the last tree prior selects a cut point,
$c_{jk}$, within the range of observed values for $x_j$; this prior is
uniform
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART error variance prior: $\wrap{\sd^2}$}
\begin{itemize}
\item The prior for $\sd^2$ is the
conjugate scaled inverse Chi-square distribution, i.e.,
$ \lambda\nu \IC{\nu}$
\item Equivalent to the inverse Gamma, i.e., 
 $\IG{\nu/2}{\lambda\nu/2} \where \E{\sd^2} = \lambda\nu/(\nu-2)$
\item We recommend that the degrees of freedom,
$\nu$, be from 3 to 10 and the default is 3 (can be over-ridden
by the argument \code{sigdf})
\item The $\lambda$ parameter can be specified by
the \code{lambda} argument (defaults to \code{NA})
\item If
\code{lambda} is unspecified, then we determine a reasonable value for
$\lambda$ based on an estimate, $\widehat\sd$, (which can be specified by
the argument \code{sigest} and defaults to \code{NA})
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The BART error variance prior}
\begin{itemize}
\item If
\code{sigest} is unspecified, the default value of \code{sigest} is
determined via linear regression or the sample standard deviation: if
$P<N$, then $y_i ~\N{\bm{x}_i'\widehat{\bm{\beta}}}{\widehat{\sd}^2}$;
otherwise, $\widehat{\sd}=s_y$
\item Now we solve for $\lambda$ such that
$\P{\sd^2\le \widehat{\sd}^2}=q$
\item This quantity, $q$, can be specified by
the argument \code{sigquant} and the default is 0.9 whereas we also
recommend considering 0.75 and 0.99
\item Note that the pair $(\nu, q)$
are potential candidate parameters for choice via cross-validation.
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function} and\\
\textcolor{red}{Marginal Effects of Independent Variables}}

Friedman 2001 {\it Annals of Statistics} \\
\begin{align*}
{f}(\bm{x})=f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})& \mbox{\qquad 
a complex function like BART where\ }
 \bm{x}=[\textcolor{red}{\bm{x}_S}, {\bm{x}_C}] \\
 & \\
f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S})&
=\E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|\textcolor{red}{\bm{x}_S} } 
%=\E{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|
%\textcolor{blue}{\mathrm{do}}(\textcolor{red}{\bm{x}_S}) } \\
\\ & \\
& \approx N^{-1} \sum_i f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}})
 \mbox{\quad partial dependence function} \\
 & \\
f_{\textcolor{red}{S}\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}) &
 \equiv N^{-1} \sum_i f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, 
{\bm{x}_{iC}}) \\
 & \\
\hat{f}_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) &
 \equiv M^{-1} \sum_m f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}) \\
\end{align*}

\end{frame}

\begin{frame}\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function} and\\
\textcolor{red}{Marginal Effects of Dependent Variables}}

\begin{itemize}
\item Consider our growth chart for height example
\item Age and weight obviously co-vary
\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
$\textcolor{red}{f_{t,\,u}(t, u)}=\E[v,w]{f(t, u, v, w)|t, u}$ 
% \item $a$ for age, $g$ for gender, $r$ for race and $w$ for weight\\
% $\textcolor{red}{f_{a,g}(a, g)}=\E[r,w]{f(a, g, r, w)|a, g}$ 
assuming \textcolor{red}{Independence}
%\qquad\ \qquad\ \ \textcolor{red}{Independence}
\item To do this right, first consider the likely strong
  relationship between age, gender and weight among children\\
  $\E{w|t, u}=\tilde{w}={\tilde{f}(t, u)}$
  %$\E{w|a, g}={\tilde{f}(a, g)}$
\item So estimate the relationship with a BART model
$w_i =\tilde{f}(t_i, u_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
%$w_i =\tilde{f}(a_i, g_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
\item A marginal effect more appropriate for dependent variables
\begin{align*}
\textcolor{blue}{f_{t,\,u}(t, u)}&=\E[v]{f(t, u, v, \tilde{w})|t, u, \tilde{w}=
\mathrm{E}[w|t, u]} & \mbox{assuming} \\
&=\E[v]{f(t, u, v, \tilde{f}(t, u))|t, u}  & \mbox{\textcolor{blue}{Dependence}} 
% \textcolor{blue}{f_{a,g}(a, g)}&=\E[r]{f(a, g, r, \tilde{w})|a, g, \tilde{w}=
% \mathrm{E}[w|a, g]} & \mbox{assuming} \\
% &=\E[r]{f(a, g, r, \tilde{f}(a, g))|a, g}  & \mbox{\textcolor{blue}{Dependence }} 
\end{align*}
%\item N.B. this is not necessarily unique: other ways to write this such
\end{itemize}

\end{frame}
\end{comment}

\begin{frame}\frametitle{Returning to the real data example}
\begin{itemize}
\item The CDC mainly used the US National Health 
and Nutrition Examination Survey (NHANES): waves I-III $n=12677$
%circa 1972, 1978, 1991
\item For simplicity, I used NHANES 1999-2000 annual/continuous  
\item The data set is in the BART3 package: \texttt{bmx}\\
and see the \texttt{height3.R} example in \texttt{demo}
\item 2-17 years (fractional age for months)
\item each child only measured once
\item height (cm) and weight (kg) collected
\item Check MCMC convergence with \textcolor{red}{$\max \widehat{R}<1.1$} for 
\textcolor{blue}{$\sd$}:\\ 
Vehtari, Gelman et al.\ 2021 {\it Bayesian Analysis}
%\item survey weights are available but are ignored here
\end{itemize} 
\begin{center}
\begin{tabular}{l|rr}
        & \multicolumn{1}{|c}{$n$} & \multicolumn{1}{c}{\%} \\ \hline 
Total   & 3435 \\ \hline
Males   & 1768 & 51.5 \\
Females & 1667 & 48.5 \\ \hline
White   &  800 & 23.3 \\
Black   & 1035 & 30.1 \\
Hispanic& 1600 & 46.6 
\end{tabular}
\end{center}
\end{frame}

\begin{frame}\frametitle{MCMC Convergence \texttt{post\$sigma}:
\textcolor{red}{$\max \widehat{R}=1.01$} \\
Burn-in 100, Thinning 1, Chains 8, Posterior 1000}
\begin{center}
\includegraphics{intro-sigma.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{MCMC Convergence \texttt{post\$sigma}: Auto-correlation}
\begin{center}
\includegraphics{intro-acf.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Test data: \textcolor{blue}{M} vs.\ \textcolor{red}{F}}
\begin{center}
\includegraphics{intro-test.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Data fit: \textcolor{blue}{M} vs.\ \textcolor{red}{F}}
\begin{center}
\includegraphics{intro-fit.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{95\% credible intervals: \textcolor{blue}{M} vs.\ \textcolor{red}{F}}
\begin{center}
\includegraphics{intro-interval.pdf}
\end{center}
\end{frame}

\end{document}

\begin{frame}\frametitle{Conclusion}
\begin{itemize}
\item This was an introduction to BART
\item BART is a flexible Bayesian nonparametric method for MLR
\item Supporting continuous, categorical and time-to-event outcomes
\item As an ensemble it has excellent out-of-sample performance
\item The tree branching penalty prevents over-fitting
\item The BART and HBART priors have convenient default settings
\item Using HBART for growth charts is an interesting exemplar
\item The BART/BART3 R packages are user-friendly and dependable
\item The rbart/hbart R packages are maturing
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Marginal effect of age
assuming independence of weight\\
 $H=200,\ \mbox{\texttt{numcut}}=100$}
\begin{center}
\scalebox{0.95}{\includegraphics{bart-growth.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
BART predictions for \textcolor{blue}{M} and \textcolor{red}{F}}
\begin{center}
\includegraphics{nosort-growth.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART predictions for \textcolor{blue}{M}\\
$H=300, \widetilde{H}=60, \mbox{\texttt{numcut}}=200$
% and \textcolor{red}{F}
}
\begin{center}
\scalebox{0.95}{\includegraphics{M-growth.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART predictions for \textcolor{red}{F} }
\begin{center}
\includegraphics{F-growth.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART vs.\ CDC for \textcolor{blue}{M}% and \textcolor{red}{F}
}
\begin{center}
\includegraphics{M-CDC.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART vs.\ CDC for \textcolor{red}{F}
}
\begin{center}
\includegraphics{F-CDC.pdf}
\end{center}
\end{frame}

\end{document}


\begin{comment}
\begin{frame}\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function}}

Friedman 2001 {\it AnnStat} \\
\begin{align*}
{f}(\bm{x})=f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})& \mbox{\qquad BART function where\ }
 \bm{x}=[\textcolor{red}{\bm{x}_S}, {\bm{x}_C}] \\
 & \\
{f}(\textcolor{red}{\bm{x}_S})&
=\E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|\textcolor{red}{\bm{x}_S} } \\
 & \\
& \approx N^{-1} \sum_i f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}})\\
 & \\
f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}) &
 \equiv N^{-1} \sum_i f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}) \\
 & \\
\hat{f}(\textcolor{red}{\bm{x}_S}) &
 \equiv M^{-1} \sum_m f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}) \\
\end{align*}

\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior computation for BART}
\begin{itemize}
\item
Now, let's briefly discuss the posterior computation related to the
Dirichlet sparse prior
\item If a Dirichlet prior is placed on the
variable splitting probabilities, $\bm{s}$, then its posterior samples
are drawn via Gibbs sampling with conjugate Dirichlet draws
\item The
Dirichlet parameter is updated by adding the total variable
branch count over the ensemble, $m_j$, to the prior setting,
$\frac{\theta}{P}$, i.e.,
$\wrap{\frac{\theta}{P}+m_1, \dots, \frac{\theta}{P}+m_P}$
\item In this way, the Dirichlet prior
induces a ``rich get richer'' variable selection strategy
\item
The sparsity parameter, $\theta$, is drawn on a fine grid of values
for the analytic posterior 
\item This draw only depends on $[s_1,\dots,s_P]$
\item Charley and I are experimenting with slice sampling
of $\theta$
\end{itemize}

\end{frame}
\end{comment}


\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}

Chipman, George \&  \textcolor{Maroon}{McCulloch} 2010 {\it Annals of Applied Stat}\\
(ChipGeor10): BART package on CRAN, BART3 on my github\\ 
\vspace*{0.2in}
%$y_i$s are centered\\ \vspace*{0.1in}
${ y_i={\mu}+g(x_i; \mathcal{T}_1, \mathcal{M}_1) + g(x_i; \mathcal{T}_2, \mathcal{M}_2) + \dots + g(x_i; \mathcal{T}_H, \mathcal{M}_H) + \epsilon_i}$\\
% \hspace*{0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}
% \hspace*{-0.3in}
% \reflectbox{\includegraphics[scale=0.05]{single-tree.png}}
% \hspace*{0.2in}${\cdots}$%\hspace*{-0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}

% \vspace*{0.15in}

where $H \in \{50, 100, 200\}$ and $\epsilon_i \iid \N{0}{\sd^2}$
%$\epsilon_i|(f, \sd) \iid \N{0}{\sd^2}$
\begin{align*}
\textcolor{red}{f(x_i)} &\ \ \equiv\ \ \textcolor{red}{\sum_h} \textcolor{blue}{g(x_i; \mathcal{T}_h, 
\textcolor{red}{\mathcal{M}_h})} \\
y_i&\ \ =\ \ {\mu}+ \textcolor{red}{f(x_i)} + \epsilon_i \where \epsilon_i \iid \N{0}{\sd^2}\\
%(f, \sd^2)  & \textcolor{red}{\ \prior \BART} \\
\textcolor{red}{f} & \prior \textcolor{red}{\BART}(\mu=\bar{y}, H=100, \kappa=2, ...) \\
{\sd^2}& \prior \nu \lambda\IC{\nu}  \\
%\textcolor{blue}{\mu_{hl}} & \prior \N{0}{\sd_{\mu}^2} \where \sd_{\mu}= \frac{0.5\, \mathrm{range}(y)}{\kappa \sqrt{H}} 
\textcolor{red}{\mu_{hl}|}\textcolor{blue}{\mathcal{T}_h} & \prior \N{0}{\frac{0.25\, \mathrm{range}(y)^2}{H\kappa^2 }} 
\textcolor{blue}{\mbox{\ leaves of\ }{\mathcal{T}}_h} \\
%\mbox{\ \textcolor{red}{leaves}} \\
&\ \ \in\ \ \textcolor{red}{\mathcal{M}_h}
\end{align*}
\end{frame}

\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Heteroskedastic BART (HBART)}}

\textcolor{red}{Pratola}, Chipman, George \& \textcolor{Maroon}{McCulloch} 
2020 {\it JCGS}\\
(PratChip20): the hbart R package on my github site
%2017 {\it arXiv preprint}\\

\begin{align*}
y_i&\ \ =\ \ \mu+ \textcolor{blue}{f(x_i)} 
+ \textcolor{red}{s(x_i)}\epsilon_i \where \epsilon_i \iid \N{0}{1}\\
\textcolor{red}{s^2(x_i)} &\ \ \equiv\ \ \textcolor{red}{
\prod_{{h}=1}^{\widetilde{H}} }
\textcolor{blue}{g(x_i; \widetilde{\mathcal{T}}_{{h}}, 
\textcolor{red}{\widetilde{\mathcal{M}}_{{h}}})} 
\where \widetilde{H} \in \{10, 20, 40\} \\
\textcolor{blue}{f} & \prior \textcolor{blue}{\BART}(\mu=\bar{y}, H=100, 
\kappa=\textcolor{blue}{5}, ...)\\
\textcolor{red}{s} & \prior \textcolor{red}{\HBART}(\widetilde{H}=20, \nu, \lambda, ...) \\
\textcolor{red}{\sd^2_{{h}l}}|\textcolor{blue}{\widetilde{\mathcal{T}}_h}& 
\prior \nu \lambda\IC{\nu} 
\textcolor{blue}{\mbox{\ leaves of\ }\widetilde{\mathcal{T}}_h} \\
&\ \ \in\ \ \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}}
\end{align*}
\end{frame}

\end{document}

\begin{frame}\frametitle{Human vessels extracted from adipose tissue}

\begin{itemize}
\item For about 20 years, Froedtert/MCW have been accepting donations
of surgically removed tissue
\item These are held in a research tissue bank
\item Researchers in the Cardiovascular Center extract the vessels
\item At low doses of Acetylcholine (ACh), the vessels are at rest
\item However, as ACh is experimentally increased, the vessels dilate
\item Dilation is measured as a percentage of the maximum dilation
\item Occasionally, constriction occurs, i.e., dilation can be $<0$
\item But, values $>100$ are impossible: a data boundary
\item Many characteristics of the donors are collected such as 
gender, race/ethnicity, age, comorbid conditions, etc.
\item For example, how is age related to vessel dilation? 
\item Three BART models to try: BART, HBART and\\ mxBART: 
Spanbauer and Sparapani 2021 {\it Stat in Med}  
(SpanSpar21): the mxBART R package on my github site
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Human vessels: three BART models}

\begin{itemize}
\item The data is in the examples/vessels subdirectory: ach\_human\_adipose.csv
\item The analyses are included too: 
age1.R, age2.R. age3.R
\item There were $i=1, \dots, N=127$ donors
\item Each vessel is measured at the following doses of ACh (moles/L):
$10^{-9}, \dots, 10^{-5}$ indexed by $j=1, \dots, 5$ 
\end{itemize}
\begin{align*}
 & & \mbox{BART models} \\
y_{ij} & = \mu+f_1(x_{ij})+\epsilon_{1ij} 
\where \epsilon_{1ij} \ind \N{0}{w_{ij}^2 \sd^2} & \mbox{Weighted} \\
y_{ij} & = \mu+f_2(x_{ij})+u_i+\epsilon_{2ij}     & \mbox{Mixed}  \\
& \where u_i \perp \epsilon_{2ij},\ 
u_i \iid \N{0}{\sd_u^2} \\ %& \mbox{SpanSpar21 2001 {\it Stat in Med}}\\
& \mbox{\ and\ } \epsilon_{2ij} \ind \N{0}{\sd^2} \\
y_{ij} & = \mu+f_3(x_{ij})+s(x_{ij})\epsilon_{3ij}
\where \epsilon_{3ij} \iid \N{0}{1} & \mbox{Heteroskedastic} \\
\end{align*}

\end{frame}

\begin{frame}\frametitle{Weighted BART}
\begin{center}
\includegraphics{age1.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Mixed BART}
\begin{center}
\includegraphics{age2.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Heteroskedastic BART}
\begin{center}
\includegraphics{age3.pdf}
\end{center}
\end{frame}

\end{document}

\begin{frame}\frametitle{Installing BART R packages}

\begin{itemize}
\item There was a bug in the hbart package that is now
fixed so you will have to download bnptools-master.zip
again to install it
\item There is NO bug in the BART3 package: it installs fine
on Linux and macOS, but not Windows 
\item The bug appears to be in R for Windows and I have
replicated the issue on a Windows laptop that Chris loaned me
\item And I have a workaround
\item Although, this workaround is only needed on Windows,
it might be of interest to those on other platforms since
it provides some insight into how R builds/installs packages
\item See the following slides
\item If you have any issues with these instructions, then
please let me know
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Installing BART R packages}

\begin{itemize}
\item Where does R install packages?
\item This is dictated by the {\tt .libPaths} function\\
issue the following to see which directory/directories\\
$>$ {\tt .libPaths() } 
\item On all platforms, there can be permissions issues
\item The last directory listed
is the global directory and you will typically not have
permission to install R packages there
\item The first directory is the default location\\
so if there is only one directory can run into permission issues
\item So you can create your own directory
\item R recognizes the following location as your HOME
directory
\item {\tt /home/USERNAME} on macOS/Linux (or {\tt $~$} for short) \\
{\tt c:$\backslash$Users$\backslash$USERNAME} on Windows
 (there is no shortcut)
\item Let's create the {\tt Rlib} directory in HOME on Windows\\
{\tt cd c:$\backslash$Users$\backslash$USERNAME}\\
{\tt c:}\\
{\tt mkdir Rlib} \\
{\tt cd} and {\tt mkdir} are roughly the same command on all platforms
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Installing BART R packages}

\begin{itemize}
\item Now create a file namede {\tt .Rprofile} in HOME
with one of the following lines so that the {\tt Rlib} is the
default location to install R packages\\
{\tt .libPaths("$~$/Rlib") \#\# macOS/Linux syntax}\\
{\tt .libPaths("c:/Users/USERNAME/Rlib") \#\# Windows}
\item I also like to specify the default CRAN mirror in this file
by the most complete and reliable mirror that I have found\\
add the following line\\
{\tt options(repos=c(CRAN="http://lib.stat.cmu.edu/R/CRAN"))}
\item Now when you start a new R session these settings should
be operational: to check type\\
$>$ {\tt .libPaths() }
\item If that is ok, then install the dependencies for the BART
packages from CRAN
\end{itemize}
$>$ {\tt install.packages("RcppEigen", dependencies=TRUE)}\\
$>$ {\tt install.packages("abind", dependencies=TRUE)}
\end{frame}

\begin{frame}\frametitle{Installing BART R packages}
\begin{itemize}
\item Download {\tt bnptools-master.zip} from 
\url{https://github.com/rsparapa/bnptools}
\item Extract the archive into some directory: it will contain the 
{\tt BART3}, {\tt mxBART} and {\tt hbart} sub-directories
\item Build the BART3 package\\
{\tt R CMD build --no-build-vignettes BART3}
\item I typically build without the vignettes that take a long
time, but you can find the vignette in the lit directory: SparSpan21
\item And install it:
{\tt R CMD INSTALL BART3\_4.3.tar.gz}
%\item You should see preprocessor statements like 
\end{itemize}
You should see preprocessor statements like 
{\tt -I../inst/include/}
\begin{itemize}
\item %If you see these, then everything should work
Since the BART3 R package has {\tt Makevars} files\\
for macOS/Linux, {\tt BART3/src/Makevars.in}\\ and Windows, 
{\tt BART3$\backslash$src$\backslash$Makevars.win}
\item In the {\tt src} directory, they each contain the line\\
{\tt PKG\_CPPFLAGS = -I../inst/include/}
\item Now build/install the mxBART package, i.e.,\\
{\tt R CMD build --no-build-vignettes mxBART}\\
{\tt R CMD INSTALL mxBART\_1.1.tar.gz}
%\item If not, then this is an R bug that we well workaround
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Installing BART R packages}

\begin{itemize}
\item If you didn't see that compiler statement it is a bug
\item This R bug appears to be only a Windows issue since
everything seems to work fine on macOS/Linux
\item So I'm assuming Windows for the rest of this slide
\item To workaround create the following directory\\
{\tt mkdir C:$\backslash$Users$\backslash$USERNAME$\backslash$Rlib$\backslash$BART$\backslash$include }
\item And copy the header files there\\
{\tt xcopy /s BART3$\backslash$include 
Rlib$\backslash$BART$\backslash$include}
\item Now edit the file {\tt BART3$\backslash$DESCRIPTION}
\item Change line number 30 from ``{\tt LinkingTo: Rcpp}''
\item To ``{\tt LinkingTo: Rcpp, BART}''
\item Then re-build/re-install it\\
{\tt R CMD build --no-build-vignettes BART3}\\
{\tt R CMD INSTALL BART3\_4.3.tar.gz}
\item Now install the mxBART package normally, i.e.,\\
{\tt R CMD build --no-build-vignettes mxBART}\\
{\tt R CMD INSTALL mxBART\_1.1.tar.gz}
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Installing BART R packages}

\begin{itemize}
\item What about the ``devtools'' trick?
\item I put it in quotes because this has basically nothing to do
  with devtools and I don't know where that reputation comes from
\item This is occasionally useful, but it is typically slower\\
(although, admittedly, it is less typing)
\item Here is an example installing hbart
that works on all platforms
(this should also work for BART3/mxBART on macOS/Linux,
but not Windows due to the bug) 
\end{itemize}
\begin{verbatim}
> library(remotes) ## not library(devtools)
> install_github("rsparapa/bnptools/hbart") 
\end{verbatim}

\end{frame}

\end{document}
