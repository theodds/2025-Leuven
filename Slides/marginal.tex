\documentclass[11pt,dvipsnames,usenames,times]{beamer}
%\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}
\usepackage{graphicx}
\setbeameroption{show notes}
\usepackage[round]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{newtxtext}
\usepackage{Sweave}
\DeclareGraphicsExtensions{.ps,.eps,.pdf,.jpg,.png}
\usefonttheme{professionalfonts}
%\usefonttheme[onlymath]{serif}
\usepackage[cmintegrals,cmbraces]{newtxmath}
\usetheme{default}
\usecolortheme{dove}
\usepackage{cancel}
\usepackage{tikz}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{color}
\usepackage{pgf} %portable graphics format
\usepackage[autobold]{statex2}
\mode<presentation>
{
  %\usetheme{Warsaw}
  % or ...
  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)

  \setbeamertemplate{navigation symbols}{}
  \usefonttheme[onlysmall]{structurebold}
  %\usefonttheme{structurebold}
}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
  \setbeamertemplate{navigation symbols}{}
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\newcommand*{\BART}{\mathrm{BART}\ }
%\newcommand*{\Wei}[2]{\mathrm{Wei}\wrap[()]{#1, #2}}
\newcommand*{\HBART}{\mathrm{HBART}\ }
\newcommand*{\corr}{\mathrm{corr}}
\newcommand*{\abs}{\mathrm{abs}}
\newcommand*{\DP}[2]{\mb{\mathrm{DP}}\wrap[()]{\mb{#1,\ #2}}}
%\newcommand*{\EV}[2]{\mb{\mathrm{ExtremeValue}}\wrap[()]{\mb{#1,\ #2}}}
%\newcommand*{\Wei}[2]{\mathrm{Wei}\wrap[()]{#1, #2}}
\newcommand*{\st}[1]{\mathrm{t}\wrap[()]{#1}}
\renewcommand*{\Dir}[1]{\mathrm{D}\wrap[()]{#1}}
\newcommand*{\nst}[3]{\mathrm{t}\wrap[()]{#1, #2, #3}}
\renewcommand*{\derivf}[2]{{\frac{\mathrm{d}}{\mathrm{d}#2}}\wrap{#1}}
\newcommand*{\derivfrac}[2]{{\frac{\partial#1}{\partial#2}}}
%\newcommand*{\derivfrac}[2]{{\frac{\mathrm{d}#1}{\mathrm{d}#2}}}
\newcommand*{\Nderivfrac}[2]{{\frac{\partial^{2}#1}{\partial^{2}#2}}}
%\newcommand*{\derivfrac2}[2]{{\frac{\mathrm{d}^2#1}{\mathrm{d}^2#2}}}
%\renewcommand*{\partialf}[2]{{\frac{\partial}{\partial #2}}\wrap{#1}}
\newcommand*{\Iff}{\;\mb{\mathrm{iff}}\;}
\newcommand*{\code}[1]{{\tt #1}}
\newcommand*{\pkg}[1]{{\tt #1}}
\newcommand*{\proglang}[1]{{\tt #1}}
\renewcommand*{\Exp}[2][]{\mb{\mathrm{E}}\ifthenelse{\equal{#1}{}}{}{_{\mb{#1}}} \wrap{\mb{#2}}}
\newcommand*{\Var}[2][]{\mb{\mathrm{V}}\ifthenelse{\equal{#1}{}}{}{_{\mb{#1}}} \wrap{\mb{#2}}}
\newcommand*{\red}[1]{\textcolor{red}{#1}}% highlight substantive changes
\newcommand*{\blue}[1]{\textcolor{blue}{#1}}% highlight substantive changes
%\newcommand*{\partialf}[2]{{\frac{\partial}{\partial #2}}\wrap{#1}}
%\DeclareRobustCommand*{\V}[2][]{\mb{\mathrm{V}}\ifthenelse{\equal{#1}{}}{}{_{\mb{#1}}} \wrap{\mb{#2}}}
\newcommand*{\partialf}[3][]{{\frac{\partial\ifthenelse{\equal{#1}{}}{}{^{\mb{#1}}}}{\partial #3\ifthenelse{\equal{#1}{}}{}{^{\mb{#1}}}}}\wrap{#2}}

\definecolor{color0}{rgb}{0,0,0}
\definecolor{color1}{rgb}{1,0,0}
\definecolor{color2}{rgb}{0,1,0}
\definecolor{color3}{rgb}{0,0,1}


%\title{\textcolor{blue}{Introduction to BART and marginal effects}}
\title{\textcolor{blue}{Machine learning regression \\ 
and marginal effects inference}}
\author{Rodney Sparapani\\
Associate Professor of Biostatistics\\
\textcolor{PineGreen}{\bf Medical College of Wisconsin}
%%Copyright (c) 2017-2022 Rodney Sparapani
}
\date{
September 15, 2025
%Marquette University Colloquium
%2024 ISBA World Meeting in Venice
%04385: Advanced Bayesian Statistics
%Department of Statistics Seminar Series\\
%University of Wisconsin-Madison
}

\begin{document}

\titlepage
\begin{comment}
\begin{quote}
Funding for this research was provided, in part, by\\
the National Marrow Donor Program/Be the Match,\\
and the Office of Naval Research
\end{quote}
\end{comment}
\boldmath

\begin{frame}%\frametitle{Motivating Example: Growth Charts}
\begin{center}
\scalebox{0.924}{\includegraphics{growth-height.eps}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Motivating Example: Growth Charts}
\begin{itemize}
\item US Centers for Disease Control and Prevention (CDC) and the\\
  World Health Organization have developed growth charts for childhood
  development: \blue{height} by \red{age}, weight by age,\\ 
body mass index by age and weight by height
\item Here we will focus on \textcolor{blue}{height}, $y_t$,\\ 
by \textcolor{red}{age} in months, $t=24, \dots, 215$ (2 to 17 years old)
%which is clearly non-linear
\item CDC uses the LMS method via natural cubic splines \\
(Cole and Green 1992 {\it Statistics in Medicine})
\item Three parameters estimated by penalized maximum likelihood \\
the Box-Cox power transformation, $L_t$;\\ the mean, $M_t$; and
the coefficient of variation, $S_t$
\begin{align*}
z_t & = \left\{\begin{array}{cc} 
\frac{-1+(y_t/M_t)^{L_t}}{L_t S_t} & L_t \not= 0 \\
\frac{\log(y_t/M_t)}{S_t} &  L_t = 0 \end{array} 
\right\} ~\N{0}{1} 
\end{align*}
\item But, this only uses part of the data: just males or just females
\item What if we wanted to use all of the data?
\item Or include more information like weight and race/ethnicity?
% \item LMS is a flexible parametric estimation procedure but it is not a
%   model in the usual sense, i.e., it creates point-by-point estimates
%   that are simply smoothed together
% \item Cole TJ. The LMS method for
% constructing normalized growth
% standards. Eur J Clin Nutr 44:45–60.
% 1990.
% \item Cole TJ, Green PJ. Smoothing
% reference centile curves: The LMS
% method and penalized likelihood. Stat
% Med 11:1305–19. 1992.
%\item Let's model the data with BART an an alternative
% \item As an alternative, let's use BART\\ to model
% height as a function of age; gender; \\ 
% race/ethnicity (whites, blacks and Hispanics) and weight
% \item The CDC used three waves of the US National Health 
% and Nutrition Examination Survey (NHANES)
% \item For simplicity, just using one wave: data
% set available in the BART3 package as \texttt{bmx}
% (Body Measures Examination)
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{\bf\textcolor{blue}{What is Artificial Intelligence and Statistical Learning?}}
\boldmath
{\it Artificial intelligence} (AI) is a computer system's ability to
  perform tasks that normally require human intelligence such as
  driving a car 
\begin{itemize}
\item 1941 (circa): ``Machine Intelligence'' coined by Alan Turing
\item 1950: Turing's {\it Imitation Game} (alike today's {\it Turing Test})
\begin{comment}
For example, player A is a computer pretending to be a man, player B is a woman and player C (who plays the role of the interrogator) can be either. In the Imitation Game, player C is unable to see either player A or player B (and knows them only as X and Y), and can communicate with them only through written notes or any other form that does not give away any details about their gender. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one
\end{comment}
\item 1956: ``Artificial Intelligence'' coined at Dartmouth Workshop
\item 1950 to 2010: AI 1.0, basic research with limited capabilities
\item 2011 to 2017: AI 2.0, deep learning
\item 2018 to today: AI 3.0, foundation/large-language models
\item Howell, Corrado \& DeSalvo 2024 {\it JAMA}
\end{itemize}
\begin{center}
\scalebox{0.75}{\includegraphics{statistical-thinking.jpg}}\\
{\it Sylwester 1993 AMSTAT News}
\end{center}
\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{What is Machine Learning (or Statistical Learning)?}}
\boldmath

\begin{itemize}
%\item In my view ``Statistical Learning'' is a better term\\ but 
%``Machine Learning'' has caught on so we are stuck with it
%\item {\it Artificial intelligence} (AI) is a computer's ability to
%  perform tasks that normally require human intelligence like driving
%a car
\item \blue{\it Machine learning}, or statistical learning, is a field
  within AI 
to develop methods that {learn} statistical relationships from\\
  {\it training data} \red{without being explicitly programmed to do so}\\
 (paraphrasing computer scientist Arthur Samuel 1959)
\item For example, you could physically model childhood growth
  chart data based on principles of human auxology or you
  could nonparametrically learn the growth curves from training data
\item Back in Samuel's day, linear/logistic regression
  were considered {\it machine learning regression (MLR)} for lack of
  alternatives; however, they do NOT meet the definition
  due to restrictive linearity and precarious parametric assumptions
\item Linear/logistic regression are proto-MLR 
rather than MLR
\item Today, by the term ``MLR'', I mean the widely flexible sense
  of \red{without being explicitly programmed to do so}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{What are black-box models?}}
\boldmath

\begin{itemize}
\item The term {\it black-box}, coined in 1945, for the development of
  an experimental analysis with electronic circuits that had been in
  practice about 20 years at that time (Belevitch 1962)
\item Simply ignore the circuit details as-if hidden inside a {\bf black-box}\\
  instead, characterize the response output from %its corresponding
  its stimulus input\\
  via experimentation, trial and error, etc.
 %(and, likely, linear interpolation)
\item MLR's are typically black-boxes and that is a down-side\\
\red{a direct interpretation of the model itself is not evident} \\
due to complexity, so let's not bother even trying 
(in stark contrast to the trivial linear/logistic regression coefficients)
\item In modern terms, a black-box model defies
understanding via inspection of the covariates
and their associated parameters
\item Rather, an intuitive interpretation is devised by other means\\ 
such as an orchestrated sequence of covariate setting predictions
\item Therefore, the \blue{rising interest in marginal {\it
      (explainable)} effects}
\item Marginal effects are applicable to MLR in general, but here we
 focus on Bayesian Additive Regression Trees (BART) 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{\bf\textcolor{blue}{What is Machine Learning Regression (MLR)?}}
\boldmath

\begin{itemize}
\item MLR is extensible, but for the moment consider the general
  regression case of a continuous outcome with Normal errors
\begin{align*}
y_i & = \mu_0 + \textcolor{blue}{f}(x_i) + \epsilon_i 
& \where \epsilon_i \iid \N{0}{\textcolor{black}{\sd^2}}
\end{align*}
\item %\textcolor{red}{\it Ideally} 
$\textcolor{blue}{f}$ is an
  unspecified %nonparametric
  function whose form is to be {\it learned} from the training data and $x_i$
  is a vector of covariates for $i=1, \dots, N$
% \item $\textcolor{blue}{f}$ is a functional ensemble: 
% a complex model which is a simple summary of many simple models\\
% $\textcolor{blue}{f}\prior$BART is a sum of trees ensemble
\begin{comment}
\item An important modern MLR extension that we will only touch on
\begin{align*}
y_i & = \mu_0 + \textcolor{blue}{f}(x_i) + \textcolor{red}{s}(x_i)\epsilon_i 
& \where \epsilon_i \iid F_{\epsilon}
\end{align*}
\item $\textcolor{blue}{f}$ alone (or $\textcolor{blue}{f}$ and
  $\textcolor{red}{s}$) will be {\it learned}, but how?
\end{comment}
%\item $\textcolor{blue}{f}$ will be {\it learned}
following Samuel's principle via 
\textcolor{blue}{Bayesian nonparametric} models
without resorting to \textcolor{red}{precarious restrictive assumptions},\\ i.e.,
we don't want to assume linearity nor pre-specify interactions, etc.
% \item Least squares has been around for 200 years and we don't need
%   advanced computing for that beyond linear algebra: BLAS/LINPACK
%   have been around since the 1970s
% unknown ensembles\\
% $\textcolor{blue}{s}\prior$HBART is a product of trees ensemble\\
% Heteroskedastic BART (HBART)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{What is Machine Learning Regression (MLR)?}}
\boldmath

\begin{itemize}
%\item {\it Deep learning} is the best currently-known machine learning
%method of prediction where all of the covariates are of the same type, i.e., 
%they are all pixels or words or audio waves, etc.
\item \blue{\it Ensemble learning} discovered
in 1997 \\
Krogh \& Solich 1997 {\it Physical Review E}
\item
  An ensemble of {\it machines} (in our case binary trees) are fit
  simultaneously that form the basis of an aggregate
  prediction with superior performance to any
  single machine's fit\\
\item Ensembles are
 the best currently-known machine
  learning method with respect to out-of-sample predictive performance
  for so-called {\it tabular data} where all of the
  covariates are of different types, i.e., age, sex, height, weight, etc.\\
\item N.B. \red{\it Deep learning} is inferior to ensembles for tabular data\\
  for optimal artificial neural net performance, the inputs need to be
  all the same type, i.e., all pixels, words or audio waves, etc.\\

Lundberg and Erion et al.\ 2020 {\it Nature Machine Intelligence}\\
Shwartz-Ziv and Armon 2022. {\it Information Fusion}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{\bf\textcolor{blue}{Why are Ensemble Learning predictions optimal?}}
\boldmath

\begin{itemize}
\item There is a trade-off between the bias and variance
\vspace*{0.1in}
\item $\textcolor{blue}{\mbox{mean squared error}= \mbox{bias}^2 + \mbox{variance}}$
\vspace*{0.1in}
\item Consider the spectrum of trade-offs\\
\vspace*{0.1in}
Linear regression is on the high bias/low variance end\\
\vspace*{0.1in}
Single-tree regression is on the low bias/high variance end\\
\vspace*{0.1in}
\item While ensemble are in between:
medium bias/medium variance
\vspace*{0.1in}
\item \textcolor{red}{BART is in the class of ensembles that
    both theoretically, and in practice, have optimal
out-of-sample predictive performance}
\end{itemize}
%Krogh \& Solich 1997 {\it Physical Review E}\\
Baldi \& Brunak 2001 ``Bioinformatics: machine learning approach''\\
Kuhn \& Johnson 2013 ``Applied Predictive Modeling''
\end{frame}

\begin{comment}
\begin{frame}[fragile]\frametitle{\bf\textcolor{black}
{Selected BART references with URLs}}
\begin{tabular}{l|l} \hline
Inception & \href{https://dx.doi.org/10.1214/09-AOAS285} 
{Chipman, George \& \red{McCulloch} 2010} {\it AOAS} \\ \hline
BART R package& {\href{https://doi.org/10.18637/jss.v097.i01}
{\blue{Sparapani}, Spanbauer \& \red{McCulloch} 2021} {\it JSS}} \\ \hline
Heteroskedastic & \href{https://dx.doi.org/10.1080/10618600.2019.1677243}
{Chipman, George \red{et al.}\ 2021} {\it Bayesian Analysis} \\ \hline
Monotonicity \& & \href{https://doi.org/10.1214/21-BA1259}
{Pratola, Chipman \red{et al.}\ 2020} {\it JCGS} \\ 
Outlier Detection & \href{https://dx.doi.org/10.1097/MPG.0000000000003492}
{\blue{Sparapani}, Teng et al.\ 2022 } {\it JPGN} \\ \hline
Variable Selection & \href{https://dx.doi.org/10.1080/01621459.2016.1264957}
{\blue{Linero} 2018} {\it JASA}\\
(Big $P$)& \href{https://doi.org/10.1080/01621459.2021.1928514}
{Liu, Rockova 2023} {\it JASA} \\ \hline
Big Data 
& \href{https://doi.org/10.1080/10618600.2013.841584}
{Pratola, Chipman \red{et al.}\ 2014} {\it JCGS}\\
(Big $N$) & \href{https://doi.org/10.1002/cjs.11343}
{Entezari, Craiu et al.\ 2017} {\it Canadian J of Stat} \\ \hline
% Efficient MCMC & \href{https://dx.doi.org/10.1214/16-BA999}
% {Pratola 2016} {\it Bayesian Analysis} \\ \hline
Skew/Multivariate & \href{https://dx.doi.org/10.1002/sim.9613}
{Um, \blue{Linero} et al.\ 2023}  {\it Statistics in Medicine} \\ \hline
Nonparametric & \href{https://proceedings.mlr.press/v89/rockova19a.html}
{Rockova \& Saha 2019} {\it PMLR} \\
Theory & \href{https://dx.doi.org/10.1214/19-AOS1879} 
{Rockova \& van der Pas 2020} {\it AOS} \\ \hline
%Propensity Scores & \href{https://doi.org/10.1214/19-BA1195} {Hahn, Murray et al.\ 2020} {\it Bayesian Analysis} \\ \hline
Survival Analysis & 
\href{https://doi.org/10.1002/sim.6893} 
{\blue{Sparapani}, Logan \red{et al.}\ 2016} {\it Statistics in Medicine}\\
%& \href{https://doi.org/10.1093/biostatistics/kxy028}
%{Henderson, Louis et al.\ 2020} {\it Biostatistics} \\
& \href{https://doi.org/10.1093/biostatistics/kxy032}
{\blue{Sparapani}, Rein et al.\ 2020} {\it Biostatistics}\\
& \href{https://doi.org/10.1177/0962280218822140}
{\blue{Sparapani}, Logan \red{et al.}\ 2020} {\it SMMR}\\
& \href{https://doi.org/10.1214/21-BA1285}
{\blue{Linero}, Basak et al.\ 2021} {\it Bayesian Analysis} \\ 
& \href{https://doi.org/10.1111/biom.13857}
{\blue{Sparapani}, Logan \red{et al.}\ 2023} {\it Biometrics} \\ \hline
\end{tabular}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{\bf\textcolor{blue}{Single-tree regression model}}
\boldmath
Chipman, George \& \red{McCulloch} 1998 {\it JASA}\\
$y_i$ is a continuous outcome where $i$ indexes subjects $i=1, \., N$\\
$x_i$ is a vector of covariates\\
$\mathcal{T}$ denotes the tree structure and branch decision rules\\
$\mathcal{M}\equiv\{{\color{red}\mu_1,\mu_2,\.,\mu_L}\}$ denotes the leaf values  \\
$g({x}_i; \mathcal{T}, \mathcal{M})$ is a regression tree function

\vspace*{-0.14in}

\begin{center}
\scalebox{0.65}{\includegraphics{single-tree.png}}
\end{center}

\vspace*{-0.1in}
$y_i=\mu_0+g({x}_i; \mathcal{T}, \mathcal{M}) + \epsilon_i \where
\epsilon_i \iid \N{0}{\sd^2}$
%$\epsilon_i|(g, \sd) \iid \N{0}{\sd^2}$
\end{frame}

\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}

Chipman, George \&  \red{McCulloch} 2010 {\it Annals of Applied Stat}\\
%(ChipGeor10): BART package on CRAN, BART3 on my github\\ 
%\vspace*{0.2in}
%${ y_i={\mu}+g(x_i; \mathcal{T}_1, \mathcal{M}_1) + g(x_i; \mathcal{T}_2, \mathcal{M}_2) + \dots + g(x_i; \mathcal{T}_H, \mathcal{M}_H) + \epsilon_i}$\\

% \hspace*{0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}
% \hspace*{-0.3in}
% \reflectbox{\includegraphics[scale=0.05]{single-tree.png}}
% \hspace*{0.2in}${\cdots}$%\hspace*{-0.1in}
% \parbox[b]{3cm}{{\includegraphics[scale=0.05]{single-tree.png}}}
% \vspace*{0.15in}

%where $H \in \{50, 100, 200\}$ and $\epsilon_i \iid \N{0}{\sd^2}$
%$\epsilon_i|(f, \sd) \iid \N{0}{\sd^2}$
\begin{align*}
y_i&\ \ =\ \ \textcolor{red}{f(x_i)} + \epsilon_i &  \epsilon_i & \iid \N{0}{w_i^2 \sd^2}\\
%(f, \sd^2)  & \textcolor{red}{\ \prior \BART} \\
\textcolor{red}{f} &\ \prior\ \textcolor{red}{\BART}(\alpha, \beta, H, \kappa, \mu_0, \tau) \\
\textcolor{red}{f(x_i)} &\ \ \equiv\ \ {\mu_0}+ \textcolor{red}{\sum_{h=1}^H} \textcolor{blue}{g(x_i; \mathcal{T}_h, \textcolor{red}{\mathcal{M}_h})} & 
H & \in \{50, \textcolor{red}{200}, 500\} \\ %$ and $\epsilon_i \iid \N{0}{\sd^2}$
%\textcolor{red}{f} & \prior \textcolor{red}{\BART}(H, \kappa=2, \mu=\bar{y},  \dots) &
%\textcolor{blue}{\mu_{hl}} & \prior \N{0}{\sd_{\mu}^2} \where \sd_{\mu}= \frac{0.5\, \mathrm{range}(y)}{\kappa \sqrt{H}} 
%\textcolor{red}{\mu(x_i)} &\ \ =\ \ \mu_0 + \textcolor{red}{f(x_i)} & \where & \red{\mu} \prior \BART \\
\textcolor{red}{\mu_{hl}|}\textcolor{blue}{\mathcal{T}_h} & \prior \N{0}{\frac{\tau^2}{4H\kappa^2 }} 
\textcolor{blue}{\mbox{\ leaves of\ }{\mathcal{T}}_h} \\
%\mbox{\ \textcolor{red}{leaves}} \\
&\ \ \in\ \ \textcolor{red}{\mathcal{M}_h} \\
{\sd^2}& \prior \lambda\nu\IC{\nu}  
\end{align*}
\end{frame}

\begin{comment}
\begin{frame}
\boldmath
\frametitle{\bf An aside: \red{MLR}, \blue{BART} and ambiguous notation}

\begin{itemize}
\item An important subtlety of MLR/BART notation that is\\
the most common pitfall of the literature/software
\item Often authors make the mistake of \\
denoting \red{$f(x)$} when they really mean \blue{$\mu+f(x)$}\\
\item I try to avoid this but it is a very easy mistake to make
\item Virtually all MLR/BART software returns \blue{$\mu+f(x)$} while\\
not properly documenting it (I have been guilty of this as well)
\item This is already bad: yet even worse for marginal effects 
\item Perhaps, we should adopt a new notation like 
$\mu(x)=\mu+f(x)$ to make the proper distinction
more evident % between \red{$f(x)$} and \blue{$\mu(x)$}
\item But, that doesn't help with what has already been published
\item So, here, I am using
\red{$f(x)$} for the BART function evaluated and \blue{$\mu+f(x)$}
for the corresponding prediction accordingly
\end{itemize}

\end{frame}
\end{comment}

\begin{comment}
\begin{frame}[fragile]
\frametitle{The {\bf BART} R package and binary trees}
\blue{Sparapani}, Spanbauer \& \red{McCulloch} 2021\\ 
{\it Journal of Statistical Software}
\begin{Sinput}
R> write(post$treedraws$trees, "trees.txt")
R> tc <- textConnection(post$treedraws$tree)
R> trees <- read.table(file=tc, fill=TRUE, row.names=NULL, header=FALSE,
+    col.names=c("node", "var", "cut", "leaf"))
R> close(tc)
R> head(trees)
\end{Sinput}
\begin{minipage}{6cm}
\begin{Soutput}
  node var cut leaf
1 1000 200   1  NA
2    3  NA  NA  NA
3    1   0  66 -0.0010
4    2   0   0  0.0048
5    3   0   0  0.0357
6    3  NA  NA  NA
\end{Soutput}
\end{minipage}
\begin{minipage}{3cm}
\usetikzlibrary{shadows}
 \begin{tikzpicture}
 [level distance=20mm,sibling distance=25mm,
   int/.style={fill=white,draw=black,drop shadow,circle,anchor=north},
   ter/.style={fill=white,rectangle,draw=black,drop shadow}]
\node[int]  {$x_1$} [grow=down]
child {node[ter] {$0.005$}
edge from parent node [left,pos=0.3] {$< c_{1,67}$}}
child {node[ter] {$0.036$}
edge from parent node [right,pos=0.3] {$\ge c_{1,67}$}};
 % \node[int]  {$x_1$} [grow=up]
 % child {node[ter] {$0.036$}
 % edge from parent node [right,pos=0.3] {$> c_{1,67}$}}
 % child {node[ter] {$0.005$}
 % edge from parent node [left,pos=0.3] {$\le c_{1,67}$}};
 \end{tikzpicture} 
\end{minipage} 
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}
\boldmath
\frametitle{\bf\textcolor{blue}{Bayesian Additive Regression Trees (BART)}}
{Logan}, \textcolor{blue}{Sparapani}, 
\red{McCulloch} \& {Laud} 2020 {\it SMMR}\\
\vspace{-15mm}
\begin{center}
\scalebox{1.42}{\includegraphics{figure1-upper.pdf}}\\
\vspace{-15mm}
\scalebox{1.42}{\includegraphics{figure1-lower.pdf}}
\end{center}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{\bf\textcolor{blue}{The BART short-hand implies the following priors}}
\boldmath
\begin{tabular}{l l}
Priors  \\ \hline
\vspace*{0.2cm}
{Covariate choice } & $\U{\{1, \., P\}}$ or \\
& $\Dir{\theta/P, \., \theta/P}$ Linero 2018 {\it JASA} \\
\vspace*{0.2cm}
{Branch decision point}   & $\U{\{1, \., C\}}$ \\
\vspace*{0.2cm}
%{Terminal node } & $\mu_{hj} ~ \N{0}{ 0.25\, \mathrm{range}(y) /H }$ \\ % $\where l \approx 16 $ \\
%{Leaf value} & $\mu_{hl} ~ \N{0}{\sd_{\mu}^2} \where \sd_{\mu}= \frac{0.5\, \mathrm{range}(y)}{\kappa \sqrt{H}}$ \\ % $\where l \approx 16 $ \\
%\vspace*{0.2cm}
%{Error variance} & ${\sd^2} ~ \nu \lambda\IC{\nu} $ \\
%\vspace*{0.2cm}
\textcolor{red}{Branching penalty} & $\P{\mbox{branch}|\textcolor{blue}{\mbox{depth}}}
= {\alpha (1+\textcolor{blue}{\mbox{depth}})^{-\beta}}$ \\ 
%$ \where \alpha \in (0, 1), \beta \ge 0$ \\
\end{tabular}

\begin{tabular}{lcccc}
Default prior settings \\
$\alpha=0.95, \beta=2$ \\ \hline
%Number of leaves & 1    & 2    & 3    & 4    & 5+   \\
Number of leaves & 1    & 2    & 3    & 4+   \\
Prior probability& 0.05 & \textcolor{red}{0.55} & \textcolor{red}{0.27} & 0.13 \\
\end{tabular}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{\bf  \textcolor{blue}{BART and Bayesian nonparametric theory}}
\boldmath

\begin{itemize}%[label=\textbullet]
\item frequentist theoretical justification for BART's performance:\\
\textcolor{red}{asymptotically consistent} with a 
\textcolor{blue}{near optimal learning rate}
\vspace*{0.1in}
\item 
the BART posterior distribution concentrates around the
truth at a \textcolor{red}{near optimal minimax rate}\\
\vspace*{0.1in}
\item 
the default BART Branching penalty is \textcolor{red}{near optimal}:\\
 $\P{\mbox{Branch}|\textcolor{blue}{\mbox{tier}}}
= a (1+\textcolor{blue}{\mbox{tier}})^{-b}$ \\
\vspace*{0.1in}
\item 
the \textcolor{red}{optimal} BART Branching penalty is now known to be:\\
 $\P{\mbox{Branch}|\textcolor{blue}{\mbox{tier}}}
= \gamma^{\textcolor{blue}{\mbox{tier}}}
\where 0<\gamma<0.5$ \\
\end{itemize}
\begin{tabular}{lcccc}
Number of leaves & 1 & 2    & 3  & 4+  \\ \hline
Prior probability& 0.00 & $(1-\gamma)^2$ & $2\gamma(1-\gamma)(1-\gamma^2)^2$ &
$\dots$ \\ 
$\gamma=0.25$    & 0.00 & 0.56 & 0.33 & 0.11 \\
$a=0.95, b=2$    & 0.05 & 0.55 & 0.27 & 0.13 \\
\end{tabular}

\vspace*{0.1in}
Rockova \& van der Pas 2019 {\it Annals of Statistics}\\
Rockova \& Saha 2019 {\it Proceedings of Machine Learning Research}\\

\end{frame}
\end{comment}

\begin{frame}\frametitle{\bf\textcolor{red}{Marginal Effects} and\\ \blue{Machine Learning Regression (MLR)}}

\begin{itemize}
\item Suppose we have an MLR, $f(x)$, that is likely a complex 
function of the covariates with nonlinearities and interactions
%generally with a large number of covariates, $P$, and interactions
\item And we divide the covariates into those of interest, $S$,
and the complement, $C$, not of interest: {$f(x) \equiv f(\red{x_S}, x_C)$}
\item Typically, $S$ is of low-dimension since we intend to peak inside the black-box by visualization:
usually 1 to 3 dimensions
\item Let $f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S})$ denote the marginal effect of $\textcolor{red}{\bm{x}_S}$
\end{itemize}
\begin{align*}
\E{y|\textcolor{red}{\bm{x}_S}} & \equiv f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) \\
f_{\red{S}}(\red{x_S}) & \equiv \E[x_C]{f(\red{x_S}, x_C)|\red{x_S}} \\
% & \equiv \E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|
% \mathrm{do}(\textcolor{red}{\bm{x}_S}) } & & \mbox{In Pearl's causal notation} \\
& = \int \cdots \int f(\red{x_S}, x_C) \wrap{x_C|\red{x_S}} \mathrm{d}x_C \\
& \where \wrap{x_C|\red{x_S}} \mbox{\ is the distribution of\ } x_C|\red{x_S}\\
& \approx \int \cdots \int f(\red{x_S}, x_C) \wrap{x_C} \mathrm{d}x_C & & \mbox{assuming\ } \red{x_S} \perp x_C 
\end{align*}

\end{frame}

\begin{frame}[fragile]\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Marginal Effects Assuming Independent Covariates}}

\begin{comment}
Suppose that we have a complex regression function, $f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})$, % ${f}(\bm{x})$, like BART
where $\textcolor{red}{\bm{x}_S} $ is a covariate subset of interest (at a fixed setting) and\\
 ${\bm{x}_{C}} $ are the complementary covariates 
\end{comment}
\begin{align*}
% {f}(\bm{x}) & =f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})&  & \where
%  \bm{x}=(\textcolor{red}{\bm{x}_S}, {\bm{x}_C})  \\
% & & & \textcolor{red}{\bm{x}_S} \mbox{\ is fixed while ${\bm{x}_{C}}$ is random} \\
\E{y|\textcolor{red}{\bm{x}_S}} & \equiv f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) & & \\
% f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) \mbox{\ is the marginal effect of\ } \textcolor{red}{\bm{x}_S} \\
f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S})&
\equiv \E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|\textcolor{red}{\bm{x}_S} } & \\
% & =\E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|
% \mathrm{do}(\textcolor{red}{\bm{x}_S}) } & & \mbox{In Pearl's causal notation} \\
%=\E{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|
%\textcolor{blue}{\mathrm{do}}(\textcolor{red}{\bm{x}_S}) } \\
& \approx N^{-1} \sum_i f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}) & &
 \mbox{ the partial dependence function} \\
& & & \mbox{where $\bm{x}_{iC}$ are the training values} \\
f_{\textcolor{red}{S}\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}) &
 \approx N^{-1} \sum_i f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, 
{\bm{x}_{iC}}) & & \mbox{for each MCMC sample\ } \textcolor{blue}{m} \\
%= N^{-1} \sum_i \sum_h g_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}; \mathcal{T}_h, \mathcal{M}_h) \\
\hat{f}_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) &
 \approx M^{-1} \sum_m f_{\textcolor{red}{S}_{\textcolor{blue}{m}}}(\textcolor{red}{\bm{x}_S}) & & \mbox{estimate by averaging over the posterior}
\end{align*}
Friedman 2001 {\it Annals of Statistics} 

\end{frame}

\begin{frame}[fragile]\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Marginal Effects Assuming Independent Covariates}}

Linear regression example: age $=x_S=x_1$, weight $=x_C=x_2$

\begin{align*}
y_i & = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i \\
f(x_{1i}, x_{2i}) & = \blue{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}} \\
%x_S & = x_1\\
%x_C & = x_2 \\
f_S(x_1) & = \E[x_2]{f(x_{1}, x_{2i})|x_1} \\
& = \E[x_2]{\beta_0 + \beta_1 x_{1} + \beta_2 x_{2i}|x_1} \\
& = \beta_0 + \beta_1 x_{1} + \beta_2 \E[x_2]{x_{2i}} \\
& \approx N^{-1} \sum_i {f(x_{1}, x_{2i})} \\
& = N^{-1} \sum_i \wrap[()]{\beta_0 + \beta_1 x_{1} + \beta_2 x_{2i}} \\
& = \red{\beta_0 + \beta_1 x_{1} + \beta_2 \b{x}_{2}} 
\end{align*}
\end{frame}

\begin{frame}[fragile]\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Marginal Effects Assuming Independent Covariates}}

Linear regression example: age $=x_S=x_1$, weight $=x_C=x_2$

\begin{center}
\scalebox{0.85}{\includegraphics{linear.pdf}}
\end{center}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{\bf\textcolor{blue}{Probit BART for dichotomous outcomes}}
\boldmath
%{Probit regression with latent variables:} Albert \& Chib 1993 {\it JASA}
\begin{align*}
y_{i}|p_{i} & \ind \B{p_{i}} \\
%p_{i}|f & =  \Phi(f(x_i)) \where f \prior \BART \\
p_{i}|f & =  \Phi(\textcolor{blue}{\mu_0}+f(x_i)) \where f \prior \BART 
\mbox{\ and\ } \mu_0=\Phi^{-1}(\b{y}) \\
% & \\
%y_i|z_i &=\I{z_i>0} \\
z_{i}|y_{i},f & ~ \N{\mu_0+ f(x_i)}{1} \begin{cases}
\I{-\infty, 0} & \If y_{i}=0 \\
\I{0, \infty} & \If y_{i}=1 \\
\end{cases}  \\
% & \\
\textcolor{red}{f|z_i,y_i} & \textcolor{red}{\;\stackrel{d}{=} f|z_i}
 % \mbox{\qquad Albert \& Chib 1993 {\it JASA}} \\
% & \\
%\wrap{\bm{y}|f} & = \prod_{i=1}^N p_{i}^{y_i} (1-p_{i})^{1-y_i} \qquad \mbox{Likelihood}
\end{align*}
\textcolor{blue}{Continuous BART} with unit variance, $\sigma^2=1$ where $z_i$ are the data\\
Albert \& Chib 1993 {\it JASA}
\end{frame}

\begin{frame}[fragile]\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Marginal Effects Assuming Independent Covariates}\\
Probit BART}

%Friedman 2001 {\it AnnStat} \\
\begin{align*}
%{p}(\bm{x})=p(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})& \mbox{\qquad BART function where\ }
% \bm{x}=[\textcolor{red}{\bm{x}_S}, {\bm{x}_C}] \\
{p}(\bm{x}) &=p(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})\\ 
&=\Phi(\mu_0+f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})) \\
p_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S})&
=\E[{\bm{x}_C}]{p(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|\textcolor{red}{\bm{x}_S} } \\
& \approx N^{-1} \sum_i p(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}) \\ &
\equiv N^{-1} \sum_i \Phi(\mu_0+f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}))\\
p_{\textcolor{red}{S}_{\textcolor{blue}{m}}}(\textcolor{red}{\bm{x}_S}) &
 \equiv N^{-1} \sum_i p_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, {\bm{x}_{iC}}) \\
\hat{p}_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_S}) &
 \equiv M^{-1} \sum_m p_{\textcolor{red}{S}_{\textcolor{blue}{m}}}(\textcolor{red}{\bm{x}_S}) \\
\end{align*}

\end{frame}
\end{comment}

\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Independent Covariates:\\ Simple Random Sampling (SRS) marginal}}
%The Conditional Means Marginal
Lundberg \& Lee 2017 {\it NIPS} \\
Janzing et al.\ 2020 {\it PMLR} 
\begin{itemize}
\item To speed up FPD when $N$ is large
\item Consider our growth chart for height example\\
age $=t$, sex $=u$, race $=v$ and weight $=w$\\
$y_i = f(t_i, u_i, v_i, w_i) + \epsilon_i \where f \prior \BART$
\item $S=(t, u)$ and $C=(v, w)$
\item Like FPD, but with $K$ random draws of $(v_i, w_i)$ from training
\item $K$ does not have to be large, e.g., $K=30$ is often sufficient
\end{itemize}
\begin{align*}
\textcolor{blue}{f_{t,\,u}(t, u)}&=\E[v,w]{f(t, u, v, w)|t, u} \\ %& \mbox{assuming} \\
&\approx K^{-1} \sum_k f(t, u, v^*_k, {w}^*_k) \\ %& \mbox{\textcolor{blue}{Dependence}} \\
\where& (v^*_k, {w}^*_k) ~ \{(v_1, w_1), \dots, (v_N, w_N)\} & & \mbox{simple random sample} \\
\end{align*}

\end{frame}

\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Dependent Covariates: the Monte Carlo (MC) marginal}}
%The Conditional Means Marginal

\begin{itemize}
\item Consider our growth chart for height example\\
age $=t$, sex $=u$, race $=v$ and weight $=w$\\
$y_i = f(t_i, u_i, v_i, w_i) + \epsilon_i \where f \prior \BART$
%\item Age and weight obviously co-vary that is not ignorable
%\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
%$\textcolor{red}{f^{\perp}_{t,\,u}(t, u)}=\E[v,w]{f(t, u, v, w)|t, u}$ 
%assuming \textcolor{red}{Independence}
\item Next we model the strong relationship between covariates\\
  $\E{w|t, u, v}=\tilde{w}=\tilde{f}(t, u, v)$
  %$\E{w|a, g}={\tilde{f}(a, g)}$
\item Here we summarize this relationship with a BART model
$w_i=\tilde{f}(t_i, u_i, v_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
%$w_i =\tilde{f}(a_i, g_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
\item For marginal effects applicable to dependent variables
\item $S=(t, u)$ and $C=(v, w)$
\end{itemize}
\begin{align*}
\textcolor{blue}{f_{t,\,u}(t, u)}&=\E[v,w]{f(t, u, v, w)|t, u} \\ %& \mbox{assuming} \\
&\approx K^{-1} \sum_k f(t, u, v^*_k, {w}^*_k) \\ %& \mbox{\textcolor{blue}{Dependence}} \\
\where& v^*_k ~ \{v_1, \dots, v_N\} & & \mbox{simple random sample} \\
\mbox{and\ }& {w}^*_k ~ \N{\tilde{f}(t, u, v^*_k)}{\sd^2_{\tilde{\epsilon}}}  
%& \where \tilde{f}_k(t, u) \mbox{\ are draws from the posterior}
% \textcolor{blue}{f_{a,g}(a, g)}&=\E[r]{f(a, g, r, \tilde{w})|a, g, \tilde{w}=
% \mathrm{E}[w|a, g]} & \mbox{assuming} \\
% &=\E[r]{f(a, g, r, \tilde{f}(a, g))|a, g}  & \mbox{\textcolor{blue}{Dependence }} 
\end{align*}
%\item N.B. this is not necessarily unique: other ways to write this such
% \item \red{However, this is just performing FPD $K$ times\\
% so $K$ times more computationally demanding!}

\end{frame}

\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Dependent Covariates: a hybrid marginal}}
%The Conditional Means Marginal

\begin{itemize}
\item Consider our growth chart for height example\\
age $=t$, sex $=u$, race $=v$ and weight $=w$\\
$y_i = f(t_i, u_i, v_i, w_i) + \epsilon_i \where f \prior \BART$
%\item Age and weight obviously co-vary that is not ignorable
%\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
%$\textcolor{red}{f^{\perp}_{t,\,u}(t, u)}=\E[v,w]{f(t, u, v, w)|t, u}$ 
%assuming \textcolor{red}{Independence}
\item Next we model the strong relationship between covariates\\
  $\E{w|t, u}=\tilde{w}=\tilde{f}(t, u)$
  %$\E{w|a, g}={\tilde{f}(a, g)}$
\item Here we summarize this relationship with a BART model
$w_i=\tilde{f}(t_i, u_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
%$w_i =\tilde{f}(a_i, g_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
\item For marginal effects applicable to dependent variables
\item $S=(t, u)$ and $C=(v, w)$
\end{itemize}
\begin{align*}
\textcolor{blue}{f_{t,\,u}(t, u)}&=\E[v,w]{f(t, u, v, w)|t, u} \\ %& \mbox{assuming} \\
&\approx K^{-1} \sum_k f(t, u, v^*_k, {w}^*_k) \\ %& \mbox{\textcolor{blue}{Dependence}} \\
\where& v^*_k ~ \{v_1, \dots, v_N\} & & \mbox{simple random sample} \\
\mbox{and\ }& {w}^*_k ~ \N{\tilde{f}(t, u)}{\sd^2_{\tilde{\epsilon}}}  
%& \where \tilde{f}_k(t, u) \mbox{\ are draws from the posterior}
% \textcolor{blue}{f_{a,g}(a, g)}&=\E[r]{f(a, g, r, \tilde{w})|a, g, \tilde{w}=
% \mathrm{E}[w|a, g]} & \mbox{assuming} \\
% &=\E[r]{f(a, g, r, \tilde{f}(a, g))|a, g}  & \mbox{\textcolor{blue}{Dependence }} 
\end{align*}
%\item N.B. this is not necessarily unique: other ways to write this such
% \item \red{However, this is just performing FPD $K$ times\\
% so $K$ times more computationally demanding!}

\end{frame}

\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Dependent Covariates:\\ the Synthetic Approximation (SA) marginal}}
Linear regression example: age $=x_S=x_1$, weight $=x_C=x_2$\\
$f_S(x_S) \approx f(x_S, \textcolor{blue}{\E[x_C]{x_C|x_S}})$
\begin{center}
\scalebox{0.8}{\includegraphics{linear2.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Dependent Covariates:\\ the Synthetic Approximation (SA) marginal}}
%The Conditional Means Marginal}

\begin{itemize}
%\item Consider our growth chart for height example
%\item Age and weight obviously co-vary that is not ignorable
%\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
%$\textcolor{red}{f^{\perp}_{t,\,u}(t, u)}=\E[v,w]{f(t, u, v, w)|t, u}$ 
% \item $a$ for age, $g$ for gender, $r$ for race and $w$ for weight\\
% $\textcolor{red}{f_{a,g}(a, g)}=\E[r,w]{f(a, g, r, w)|a, g}$ 
%assuming \textcolor{red}{Independence}
%\qquad\ \qquad\ \ \textcolor{red}{Independence}
\item We proceed as before by modeling
the strong relationship between age, sex and weight first\\
\item For marginal effects applicable to dependent variables
\item $S=(t, u)$ and $C=(v, w)$
\begin{align*}
  \E{w|t, u}& ={\tilde{f}(t, u)} \\
  %$\E{w|a, g}={\tilde{f}(a, g)}$
%\item We can summarize the relationship with a BART model
w_i & =\tilde{f}(t_i, u_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART \\
\widehat{w}(t, u) & =M^{-1}\sum_m\tilde{f}_m(t, u) \\
%$w_i =\tilde{f}(a_i, g_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
%\item For marginal effects more applicable to dependent variables
\textcolor{blue}{f_{t,\,u}(t, u)}&=\E[v]{f(t, u, v, {w})|t, u, {w}=
\mathrm{E}[w|t, u]} \\% & \mbox{assuming} \\
&=\E[v]{f(t, u, v, \widehat{w}(t, u))|t, u} \\% & \mbox{\textcolor{blue}{Dependence}} \\
&\approx N^{-1}\sum_if(t, u, v_i, \widehat{w}(t, u)) 
% \textcolor{blue}{f_{a,g}(a, g)}&=\E[r]{f(a, g, r, \tilde{w})|a, g, \tilde{w}=
% \mathrm{E}[w|a, g]} & \mbox{assuming} \\
% &=\E[r]{f(a, g, r, \tilde{f}(a, g))|a, g}  & \mbox{\textcolor{blue}{Dependence }} 
\end{align*}
%\item N.B. this is not necessarily unique: other ways to write this such
% \item \blue{Now this is just performing FPD one time\\
% so a more friendly computation!}
\end{itemize}

\end{frame}


\begin{frame}\frametitle{\bf%\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Dependent Covariates:}\\
 the Nearest Neighbor (NN) marginal}
%The Conditional Dependence Marginal}

\begin{itemize}
%\item Again consider our growth chart for height example
\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $\blue{w}$
  for weight
\item For age, $t$, we have chosen this grid of values\\
$-\infty={t}^*_{0} < {t}^*_{1} < {t}^*_{2} < \dots < 
{t}^*_{J} < {t}^*_{J+1}=\infty$ \\
$\where {t}^*_{1}=2, \dots, {t}^*_{17}=18$ \\
\item For sex, $u$, we have just two values: 
${u}^* \in \{\blue{M}, \red{F}\}$ 
%\item N.B. this is not necessarily unique: other ways to write this such
\end{itemize}
\begin{align*}
f_{\textcolor{red}{S}}(\red{{t}^*_j}, {u}^*)
& \approx K({t}^*_j, {u}^*)^{-1} \sum_{\mathcal{X}({t}^*_j,\ {u}^*)}
%& = K_{\tilde{t}_j, \tilde{u}}^{-1} \sum_{\{i: \tilde{t}_{j-1} <t_{i} < \tilde{t}_{j+1},\ u_i=\tilde{u}\}} 
f(\textcolor{red}{{t}^*_j}, {u}^*, {v}_{i}, \blue{{w}_{i}}) \\
%& \where K_{\tilde{t}_j, \tilde{u}}=|\{i: \tilde{t}_{j-1} <t_{i} < \tilde{t}_{j+1},\ u_i=\tilde{u}\}|
& \where \mathcal{X}({{t}^*_j, {u}^*})=\{i: {t}^*_{j-1} <t_{i} < {t}^*_{j+1},\ u_i={u}^*\} \\
& \mbox{\ and\ } K({t}^*_j, {u}^*)=|\mathcal{X}({{t}^*_j, {u}^*})|
\end{align*}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{ MLR marginal effects and computational efficiency }
\begin{itemize}
\item How can marginal effects be calculated efficiently with BART?
\item And beyond BART, many of the ideas that we will explore here can be readily adapted to
  other MLR methods
\item Nearest Neighbor Marginals are generally efficient, but
may not be applicable to every problem
\item For large training sets, FPD can be computationally demanding
whether assuming independence or with Direct Imputation
\item In these cases, we are seeking a faster marginal method than FPD
\item We can speed up FPD by \blue{\it random sampling} \\
Lundberg and Lee 2017 {\it NIPS}\\
 Janzing, Minorics and Blobaum 2020 {\it PMLR}
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{FPD vs.\ FPD by random sampling }

FPD
\begin{align*}
f_{\textcolor{red}{S}_{F_{\textcolor{blue}{m}}}}(\textcolor{red}{\bm{x}_S}) &
 \equiv N^{-1} \sum_i f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, 
{\bm{x}_{iC}})  & \\ & & &
  \mbox{where $\bm{x}_{iC}$ is a  training value}  \\
\hat{f}_{\textcolor{red}{S}_F}(\textcolor{red}{\bm{x}_S}) &
 \equiv M^{-1} \sum_m f_{\textcolor{red}{S}_{F_{\textcolor{blue}{m}}}}(\textcolor{red}{\bm{x}_S})
\end{align*}

FPD by random sampling
\begin{align*}
f_{\textcolor{red}{S}^K_{F_{\textcolor{blue}{m}}}}(\textcolor{red}{\bm{x}_S})
& \equiv K^{-1} \sum_k f_{\textcolor{blue}{m}}(\textcolor{red}{\bm{x}_S}, {\bm{x}_{k_{\textcolor{blue}{m}}C}}) & \\ & & &
  \mbox{$\bm{x}_{k_{\textcolor{blue}{m}}C}$ is a draw from the training} \\
\hat{f}_{\textcolor{red}{S}^K_F}(\textcolor{red}{\bm{x}_S}) &
 \equiv M^{-1} \sum_m f_{\textcolor{red}{S}^K_{F_{\textcolor{blue}{m}}}}(\textcolor{red}{\bm{x}_S})
\end{align*}
\end{frame}

\begin{frame}[fragile]\frametitle{FPD by random sampling and the empirical variance}

\begin{itemize}
\item It is clear that 
$\red{\Exp{\hat{f}_{\textcolor{red}{S}_{F}}(\textcolor{red}{\bm{x}_S})}
\approx \Exp{\hat{f}_{\textcolor{red}{S}^K_F}(\textcolor{red}{\bm{x}_S})}} $
\item However, it is also clear that the variances are not equal
\end{itemize}
\begin{align*}
\Var{\hat{f}_{S^K_F}({\bm{x}}_S)|\bm{y}} = & 
\mathrm{V}\left[\Exp{\hat{f}_{S^K_F}({\bm{x}}_S)|\hat{f}_{S_F}({\bm{x}}_S), \bm{y}}|\bm{y}\right] \\
& + \mathrm{E}\left[\Var{\hat{f}_{S^K_F}({\bm{x}}_S)|\hat{f}_{S_F}({\bm{x}}_S), \bm{y}}|\bm{y}\right] \\
=& \Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}}\\
& + \mathrm{E}\left[K^{-1}\Var{f({\bm{x}_S}, {\bm{x}_{kC}})|\hat{f}_{S_F}({\bm{x}}_S),\bm{y}}|\bm{y}\right] \\
\approx & \blue{\Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}} 
+ K^{-1}\Exp{ s^2_{S^K_F(\bm{x}_S)} |\bm{y}}} \\
\where & s^2_{S^K_F(\bm{x}_S)}=
%\where & s^2_{S_K}({\bm{x}_S})=
K^{-1} \sum_k (f({\bm{x}_S}, {\bm{x}_{kC}})-\hat{f}_{S^K_F}({\bm{x}}_S))^2 
\end{align*}
\end{frame}

\begin{frame}[fragile]\frametitle{FPD by random sampling and the empirical variance}

\begin{align*}
\Var{\hat{f}_{S^K_F}({\bm{x}}_S)|\bm{y}} 
\approx & \Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}} 
+ K^{-1}\Exp{s^2_{S^K_F(\bm{x}_S)}|\bm{y}} 
\end{align*}
\begin{itemize}
\item The first term $ \Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}} $ is the
target variance of the calculation we want to avoid
\item And the second term can be estimated from the posterior as
$ \widehat{s^2}_{S^K_F(\bm{x}_S)}=
M^{-1} \sum_m s^2_{S^K_{F_m}(\bm{x}_S)}  $
\item Therefore, we can empirically estimate the variance like so\\
$\Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}} 
\approx  \Var{\hat{f}_{S^K_F}({\bm{x}}_S)|\bm{y}}
- K^{-1} \widehat{s^2}_{S^K_F(\bm{x}_S)} $
\item So, we generate the posterior for the random sampling estimator 
$ f_{S_{F_m}}({\bm{x}}_S) \approx \hat{f}_{S^K_F}({\bm{x}}_S)+
\wrap{f_{S^K_{F_m}}({\bm{x}}_S)-\hat{f}_{S^K_F}({\bm{x}}_S)}
\sqrt{\frac{\Var{\hat{f}_{S_F}({\bm{x}}_S)|\bm{y}}}{\Var{\hat{f}_{S^K_F}({\bm{x}}_S)|\bm{y}}}} $
\end{itemize}
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}\frametitle{\bf\textcolor{blue}{Friedman's partial dependence function (FPD)} and\\
\textcolor{red}{Marginal Effects of Dependent Variables}: subset method}

Suppose for one setting, $x_{ik}$, we have an increasing grid of values\\
$-\infty=\tilde{x}_{0} < \tilde{x}_{1} < \tilde{x}_{2} < \dots < 
\tilde{x}_{J} < \tilde{x}_{J+1}=\infty$
\begin{align*}
f_{\textcolor{red}{S}}(\textcolor{red}{\bm{x}_{S_{-k}}, \tilde{x}_{j}})
& \equiv K^{-1} \sum_{\{i: \tilde{x}_{j-1} <x_{ik} < \tilde{x}_{j+1}\}} f(\textcolor{red}{\bm{x}_{S_{-k}}, \tilde{x}_{j}}, {\bm{x}_{iC}}) \\
& \where K=\sum_i \I{\tilde{x}_{j-1} <x_{ik} < \tilde{x}_{j+1}}
\end{align*}

\end{frame}
\end{comment}

\begin{frame}\frametitle{Returning to the real data example}
\begin{itemize}
\item CDC's data is the US National Health 
and Nutrition Examination Survey (NHANES) waves I-III\\ 
circa 1972 (I), 1978 (II), 1991 (III): \red{$n$=12677}
\item For simplicity, I used NHANES annual/continuous 1999-2000 
\item The data set is in the BART3 package: \texttt{bmx}\\
see the \texttt{growth*.R} examples in \texttt{demo}
\item 2-17 years (fractional age for months)
\item each child only measured once
\item height (cm) and weight (kg) collected
%\item Check MCMC convergence with \textcolor{red}{$\max \widehat{R}<1.1$} for 
%\textcolor{blue}{$\sd$}:\\ 
%Vehtari, Gelman et al.\ 2021 {\it Bayesian Analysis}
%\item survey weights are available but are ignored here
\end{itemize} 
\vspace*{-2mm}
\begin{center}
\begin{tabular}{l|rr}
        & \multicolumn{1}{|c}{$n$} & \multicolumn{1}{c}{\%} \\ \hline 
Total   & \blue{3435} \\ \hline
Males   & 1768 & 51.5 \\
Females & 1667 & 48.5 \\ \hline
White   &  800 & 23.3 \\
Black   & 1035 & 30.1 \\
Hispanic& 1600 & 46.6 
\end{tabular}
\end{center}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{MCMC Convergence \texttt{fit1\$sigma.}\\
Auto-correlation: {\tt growth0.R}}
\begin{center}
\includegraphics{growth0-acf.pdf}
\end{center}
\end{frame}


\begin{frame}\frametitle{MCMC Convergence \texttt{fit1\$sigma}:
\textcolor{red}{$\max \widehat{R}=1.05$} \\
Chains 8: {\tt growth0.R}}
\begin{center}
\includegraphics{growth0-sigma.pdf}
\end{center}
\end{frame}
\end{comment}

\begin{frame}\frametitle{$R^2=96.2\%$ in the testing subset: {\tt growth1.R}}
\begin{center}
\includegraphics{growth1-fit1.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age comparison}
\begin{center}
\scalebox{0.95}{\includegraphics{FPD-final.pdf}}
\end{center}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{Marginal effect of age: FPD
assuming weight is independent\\
\red{F} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-indF.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: Direct Imputation Marginal\\
\red{F} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-cmF.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: Nearest Neighbor Marginal\\
\red{F} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-cdF.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: FPD
assuming weight is independent\\
\blue{M} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-indM.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: Direct Imputation Marginal\\
\blue{M} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-cmM.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: Nearest Neighbor Marginal\\
\blue{M} only: {\tt growth1.R}}
\begin{center}
\scalebox{0.95}{\includegraphics{growth1-cdM.pdf}}
\end{center}
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}
\frametitle{\bf\textcolor{blue}{Heteroskedastic BART (HBART)}}

{Pratola}, Chipman, George \& \red{McCulloch} 2020 {\it JCGS}\\
%2017 {\it arXiv preprint}\\

\begin{align*}
y_i&\ \ =\ \ \mu_0+ {f(x_i)} 
+ \textcolor{red}{s(x_i)}\epsilon_i & \epsilon_i & \iid \N{0}{w_i^2 \sd^2}\\
{f} &\ \prior\ {\BART}(a, b, H, \kappa, \mu_0, \tau) \\
%\where \widetilde{H} \in \{10, 20, 40\} \\
\textcolor{red}{s^2} &\ \prior\ \textcolor{red}{\HBART}(
\tilde{a}, \tilde{b}, \widetilde{H}, \tilde{\lambda}, \tilde{\nu})
 \\
\textcolor{red}{s^2(x_i)} &\ \ \equiv\ \ \textcolor{red}{ \prod_{{h}=1}^{\widetilde{H}} } \textcolor{blue}{g(x_i; \widetilde{\mathcal{T}}_{{h}}, \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}})}  & \widetilde{H} & \approx H/5 \\
\textcolor{red}{\sd^2_{{h}l}}|\textcolor{blue}{\widetilde{\mathcal{T}}_h}& 
\prior \lambda\nu\IC{\nu} 
\textcolor{blue}{\mbox{\ leaves of\ }\widetilde{\mathcal{T}}_h} 
& \lambda & = \tilde{\lambda}^{1/\widetilde{H}}\\
\\ %& \lambda & = \tilde{\lambda}^{1/\widetilde{H}} \\
&\ \ \in\ \ \textcolor{red}{\widetilde{\mathcal{M}}_{{h}}}
& \nu & = 2 \wrap{1-\wrap[()]{1-\frac{2}{\tilde{\nu}}}^{1/\widetilde{H}}}^{-1} \\
%& \nu & = 2 \wrap{1-\wrap[()]{1-\frac{2}{\tilde{\nu}}}^{1/\widetilde{H}}}^{-1}
\end{align*}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART predictions for \textcolor{blue}{M}\\
Direct Imputation Marginal:  \bf{hbart} \tt{demo/height} %$H=300, \widetilde{H}=60$
%$, \mbox{\texttt{numcut}}=200$
% and \textcolor{red}{F}
}
\begin{center}
\scalebox{0.95}{\includegraphics{M-growth.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART predictions for \textcolor{red}{F}\\
Direct Imputation Marginal: \bf{hbart} \tt{demo/height} }
\begin{center}
\includegraphics{F-growth.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART vs.\ CDC for \textcolor{blue}{M}\\% and \textcolor{red}{F}
Direct Imputation Marginal: \bf{hbart} \tt{demo/height}}
\begin{center}
\includegraphics{M-CDC.pdf}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age: 
HBART vs.\ CDC for \textcolor{red}{F}\\
Direct Imputation Marginal: \bf{hbart} \tt{demo/height}}
\begin{center}
\includegraphics{F-CDC.pdf}
\end{center}
\end{frame}
\end{comment}

\begin{frame}\frametitle{ MLR marginal effects and computational efficiency }
\begin{itemize}

\item \blue{\it Shapley values} are a popular choice for explainability
research that are based on marginal effects
\item However, \red{Shapley values are very computationally intensive}\\
In terms of complexity, they are considered to be NP-hard:\\
 not practical unless the number of covariates is small
\item Shapley values approximate $f(x)$ by additive effects\\ 
(typically of one variable at a time), e.g., $f(x) \approx \mu+ \sum_j f_j(x_j)$
\item \blue{$f(x)$ is additive in terms of single covariate functions,
$f_j(x_j)$, i.e., effectively, we are assuming independence} 
\item But there is a common extension for two-way interactions \\
Lundberg and Erion et al.\ 2020 {\it Nature Machine Intelligence}
\item And multi-way interactions have been defined as well\\
Grabisch \& Roubens 1999; Borgonovo et al.\ 2024
%: they may be
%intuitive for game theory, but that does not readily transfer to MLR
%\item Another limitation, Shapley values are not uniquely well-defined
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{ Marginal effects and computational efficiency }
\begin{itemize}
\item We can speed up FPD with \blue{\it random sampling} that we call {FPDK}
Lundberg and Lee 2017; Janzing, Minorics and Blobaum 2020

\item My {\bf BART3} package on {\tt github} has S3 methods for
{\tt FPD}\\
and its countpart with random sampling: {\tt FPDK}
\end{itemize}
%\begin{verbatim}
{\tt 
R> install.packages("Rcpp", dependencies=TRUE)\\
R> install.packages("remotes", dependencies=TRUE)\\
R> library("remotes")\\
R> install\_github("rsparapa/bnptools/BART3")}
%\end{verbatim}
\end{frame}



\begin{frame}\frametitle{Marginal effect of age for \textcolor{blue}{M} and \textcolor{red}{F}: {\tt growth2.R}} 
\begin{center}
%\scalebox{0.95}{\includegraphics{growth2-FPDK.pdf}}
{\includegraphics{growth2-FPDK.pdf}}
\end{center}
\end{frame}

%\begin{frame}\frametitle{Marginal effect of age for \textcolor{blue}{M} and \textcolor{red}{F}: {\tt growth2.R}} 
\begin{frame}\frametitle{Marginal effect 95\% credible intervals: {\tt growth2.R}} 
\begin{center}
{\includegraphics{growth2-FPDK-EV.pdf}}
%\scalebox{0.96}{\includegraphics{growth2-FPDK-EV.pdf}}
\end{center}
\end{frame}
\end{comment}

\begin{frame}[fragile]\frametitle{Shapley value marginal effects of \blue{Independent Covariates}}
\begin{itemize}

\item {\bf Two equivalent definitions: original ordered vs.\ \\
more computationally friendly unordered}
\item $S=\{x_j\}$ and $C_{-j}$ contains all $P-1$ other covariates
\item $\mathcal{P}_j$ is the set of all {\it ordered} permutations of
  $S \cup C_{-j}$\\
  $f_j(x_j) \equiv (P!)^{-1} \sum_{S_* \in \mathcal{P}_j}
  [f_{+j}^{*}(x_{S_*}) - f_{-j}^{*}(x_{S_*})] $\\
where $f_{+j}^{*}(x_{S_*})$ evaluates arguments up to/including $x_j$\\
while $f_{-j}^{*}(x_{S_*})$ evaluates arguments before/excluding $x_j$
\item $\mathcal{C}_j$ is the set of all {\it unordered} combinations $S_* \subset C_{-j}$\\
$f_j(x_j) \equiv  \sum_{S_* \in \mathcal{C}_j}
  \frac{|S_*|!(P-|S_*|-1)!}{P!}
  [f_{S}(x_{S_*}, x_j) - f_{S}(x_{S_*})] $
\item If each $ f_{S}(.) $ are fit from the training\\ 
\red{the number of fits needed grows rapidly with $P$}
\end{itemize}
\begin{tabular}{crrrrrrrc} \hline
$P$  & 2 & 3 &  4 &  5 &   10 &      20 &         30 & $P$ \\
Fits & 3 & 7 & 15 & 31 & 1,023 & 1,048,575 & 1,073,741,823 &$2^P-1$ \\ \hline
\end{tabular}
 

\end{frame}


\begin{frame}[fragile]\frametitle{Fast Shapley value approximations from a single fit}
\begin{itemize}

\item Rather than fitting so many models, 
Shapley values can be created from a single fit's marginal effects
%\blue{\it marginal effect recursion}
% \item $f_j(x_j) \equiv \sum_{C \subset F} \frac{|C|!(P-2)!}
% {P!} [f_{S,C}(x_{x_S, x_C}) - f_{S}(x_{S})] $
% \item Now each $ f_{S,C}(x_{S},x_C) $ and $ f_{S}(x_{S}) $ are 
% marginal effects estimated from the single overall fit $f(x)$
\item For example, suppose $ f_{S}(x_{S}) = 
\E[{\bm{x}_{C}}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})|\textcolor{red}{\bm{x}_S} } $ 
%& =\E[{\bm{x}_C}]{f(\textcolor{red}{\bm{x}_S}, {\bm{x}_{C}})| \mathrm{do}(\textcolor{red}{\bm{x}_S}) }
\item This would certainly help but the computations are still daunting
unless the number of covariates is small
\item There is a simple algorithm, known as {\tt EXPVALUE}, for these marginals
that is basically equivalent to FPD\\
% (Lundberg and Erion et al. 2020)
%\item 
And there are more efficient, so-called Tree SHAP, algorithms
but these are far more complex to program\\
Lundberg and Erion et al.\ 2020 {\it Nature Machine Intelligence}
%\item But SHAP is still pretty slow
%\item Or we can \blue{use random sampling: what I call SHAPK}
\item And advanced random sampling schemes have been
proposed 
but they are challenging to implement as well\\
Yang, Zhou et al.\ 2023 {\it JASA}
%\item I'm not going to show the details but it is a simple extension like 
%going from FPD to FPDK
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Shapley value marginals for \blue{Dependent Covariates}\\
Marginal effect of age}

\begin{itemize}
\item Shapley values come from game theory where\\ 
each player takes their turn and the order of play is important
\item The {\it players} here are the covariates
\item And as can be shown, the order of covariates doesn't really matter\\
i.e., the order of covariates is arbitrary (Lundberg and Lee 2017)
\item Nevertheless, all possible orderings of $t,u,v,w$: $P! = 24$
\end{itemize}
\begin{center}
\begin{tabular}{llll}
\multicolumn{1}{c}{age} & \multicolumn{1}{c}{age} & 
\multicolumn{1}{c}{age} & \multicolumn{1}{c}{age} \\
\multicolumn{1}{c}{first} & \multicolumn{1}{c}{second} & 
\multicolumn{1}{c}{third} & \multicolumn{1}{c}{last} \\
$t,u,v,w$ & $u,t,v,w$ & $u,v,t,w$ & $u,v,w,t$ \\
$t,u,w,v$ & $u,t,w,v$ & $u,w,t,v$ & $u,w,v,t$ \\
$t,v,u,w$ & $v,t,u,w$ & $v,u,t,w$ & $v,u,w,t$ \\
$t,v,w,u$ & $v,t,w,u$ & $v,w,t,u$ & $v,w,u,t$ \\
$t,w,u,v$ & $w,t,u,v$ & $w,u,t,v$ & $w,u,v,t$ \\
$t,w,v,u$ & $w,t,v,u$ & $w,v,t,u$ & $w,v,u,t$ \\
\end{tabular}
\end{center}
\end{frame} 

\begin{frame}
\frametitle{Shapley value marginal effects of \blue{Dependent Covariates}\\
Marginal effect of age}

Differentials for $t$ corresponding to each ordering

\begin{tabular}{llll}
$\scriptstyle f(t)-0$ & $\scriptstyle f(u,t)-f(u)$ & $\scriptstyle f(u,v,t)-f(u,v)$ & $\scriptstyle f(u,v,w,t)-f(u,v,w)$ \\
$\scriptstyle f(t)-0$ & $\scriptstyle f(u,t)-f(u)$ & $\scriptstyle f(u,w,t)-f(u,w)$ & $\scriptstyle f(u,w,v,t)-f(u,w,v)$ \\
$\scriptstyle f(t)-0$ & $\scriptstyle f(v,t)-f(v)$ & $\scriptstyle f(v,u,t)-f(v,u)$ & $\scriptstyle f(v,u,w,t)-f(v,u,w)$ \\
$\scriptstyle f(t)-0$ & $\scriptstyle f(v,t)-f(v)$ & $\scriptstyle f(v,w,t)-f(v,w)$ & $\scriptstyle f(v,w,u,t)-f(v,w,u)$ \\
$\scriptstyle f(t)-0$ & $\scriptstyle f(w,t)-f(w)$ & $\scriptstyle f(w,u,t)-f(w,u)$ & $\scriptstyle f(w,u,v,t)-f(w,u,v)$ \\
$\scriptstyle f(t)-0$ & $\scriptstyle f(w,t)-f(w)$ & $\scriptstyle f(w,v,t)-f(w,v)$ & $\scriptstyle f(w,v,u,t)-f(w,v,u)$ 
\end{tabular}

Weighted differentials for $t$ corresponding to each ordering
\begin{tabular}{llll}
$\scriptstyle 6{f(t)}$ & $\scriptstyle 2\wrap{ f(t,u)-f(u)}$ & 
$\scriptstyle 2\wrap{f(t,u,v)-f(u,v)}$ & $\scriptstyle 6\wrap{f(t,u,v,w)-f(u,v,w)}$ \\
 & $\scriptstyle 2\wrap{ f(t,v)-f(v)}$ & $\scriptstyle 2\wrap{f(t,u,w)-f(u,w)}$ & \\
 & $\scriptstyle 2\wrap{f(t,w)-f(w)}$ & $\scriptstyle 2\wrap{f(t,v,w)-f(v,w)}$ & \\ \hline
0 & 1 & 2 & 3 \\ \hline
$3!$ & $2!$ & $2!$ & $3!$
\\ \hline
\end{tabular}
Last row are the weights for the differentials: $|S_*|!(P-|S_*|-1)!$
(Lundberg and Lee 2017)
\end{frame} 

\begin{frame}\frametitle{\bf Shapley values and\\
\textcolor{red}{Marginal Effects for Dependent Covariates:}\\
Synthetic Approximation marginal}
%\\ \blue{Conditional Means Marginal}}}

\begin{itemize}
%\item Once again consider our growth chart for height example
%\item Ignore age by sex for simplicity: let's just consider age
%\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
%$\textcolor{red}{f_{t}(t)}=\E[u,v,w]{f(t, u, v, w)|t}$ 
%assuming \textcolor{red}{Independence}
%\item The marginal effect is  $f_t(t)$
%that has a poor fit with the data\\
%similar to that of FPD assuming independence
\item As before, rely on the strong
  relationships of age, sex and weight\\
  $\E{w|t, u}={\tilde{f}(t, u)}$\\
%  $\E{w|t}=\tilde{w}_.={\tilde{f}_.(t)}={0.5 \wrap{\tilde{f}(t, \blue{M})+\tilde{f}(t, \red{F})}}$\\
$w_i =\tilde{f}(t_i, u_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
\item For a marginal effect more applicable to dependent variables\\
% e.g., \red{\it F}:
Females: $\mu+f_t(t)+f_u(\red{F})+2f_{t:u}(t, \red{F})+f_w({\widehat{w}(t, \red{F})})$
\end{itemize}

\end{frame}

\begin{comment}
\begin{frame}\frametitle{\bf Shapley values and\\
\textcolor{red}{Marginal Effects for Dependent Covariates}}
%\\ \blue{Conditional Means Marginal}}}

\begin{itemize}
\item Once again consider our growth chart for height example
%\item Age and weight obviously co-vary
%\item But, it is not clear how to create marginal effects
%of age by sex\\ 
\item Ignore age by sex for simplicity: let's just consider age
\item $t$ for age, $u$ for sex, $v$ for race/ethnicity and $w$ for weight\\
$\textcolor{red}{f_{t}(t)}=\E[u,v,w]{f(t, u, v, w)|t}$ 
% \item $a$ for age, $g$ for gender, $r$ for race and $w$ for weight\\
% $\textcolor{red}{f_{a,g}(a, g)}=\E[r,w]{f(a, g, r, w)|a, g}$ 
assuming \textcolor{red}{Independence}
%\qquad\ \qquad\ \ \textcolor{red}{Independence}
\item The marginal effect assuming independence is just $f_t(t)$\\
that has a poor fit with the data (like an FPD ignoring weight)
\item Estimate the strong
  relationship between age and weight\\
  $\E{w|t, u}=\tilde{w}=\mu_w+{\tilde{f}(t, u)}$\\
  $\E{w|t}=\tilde{w}_.={\tilde{f}_.(t)}={0.5 \wrap{\tilde{f}(t, \blue{M})+\tilde{f}(t, \red{F})}}$\\
  %$\E{w|a, g}={\tilde{f}(a, g)}$
%\item We can summarize the relationship with a BART model
$\where w_i =\mu_w+\tilde{f}(t_i, u_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
%$w_i =\tilde{f}(a_i, g_i)+\tilde{\epsilon}_i \where \tilde{f} \prior \BART$
\item A more appropriate marginal effect for dependent variables\\
$f_t(t)+f_w({\tilde{w}_.})=f_t(t)+f_w({\tilde{f}_.(t)})$
%\item N.B. this is not necessarily unique: other ways to write this such
\end{itemize}

\end{frame}


\begin{frame}\frametitle{Marginal effect of age with or without weight\\
 FPD vs.\ SHAP for \textcolor{blue}{M} and \textcolor{red}{F}: {\bf BART3} {\tt demo/growth4}} 
\begin{center}
\scalebox{0.95}{\includegraphics{SHAP-final5.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effects: FPD vs.\ SHAP} 
\begin{center}
\scalebox{0.95}{\includegraphics{growth3-SHAP.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age with/without weight\\
 FPD (\textcolor{blue}{M} and \textcolor{red}{F}) vs.\ SHAPK} 
\begin{center}
\scalebox{0.95}{\includegraphics{growth4-SHAPK.pdf}}
\end{center}
\end{frame}

\begin{frame}\frametitle{Marginal effect of age with FPD vs.\ SHAPK for M only\\
95\% credible interval: \textcolor{blue}{97.5\%} and \textcolor{red}{2.5\%} quantiles} 
\begin{center}
\scalebox{0.96}{\includegraphics{growth4-SHAP-M.pdf}}
\end{center}
\end{frame}
\end{comment}

\begin{frame}\frametitle{Marginal effects with Shapley values} 
\begin{center}
\scalebox{0.95}{\includegraphics{SHAP-final5.pdf}}
\end{center}
\end{frame}

\begin{comment}
\begin{frame}\frametitle{Shapley interaction index: NEEDS WORK}
\begin{itemize}
\item The Shapley interaction index approximates $f(x)$ by additive
  effects with two-way interactions:
  $f(x) \approx \sum_j\sum_k f_{x_j,\,x_k}(x_j, x_k)$\\
  \red{(N.B. yet another approximation of $f(x)$ so it is not uniquely
    well-defined, e.g., what about three-way interactions, etc.?)}
\item Consider all possible combinations of the unordered subsets for
  the $x$ variables $S \in F$ where $F=\{1, \dots, P\}$ 
\item If $j =k$: $f_{x_j,\,x_j}(x_j, x_j)= f_{x_j}(x_j)- \sum_{k\not =j} f_{x_j,\,x_k}(x_j,x_k)$
\item If $j\not =k$: $f_{x_j,\,x_k}(x_j,x_k) \equiv \sum_{S \subset F\setminus\{j,\,k\}} 
\frac{|S|!(P-|S|-2)!}
{2(P-1)!} \tilde{f}_{x_j,\,x_k}(x_j,x_k) $ 
\begin{align*}
\where \tilde{f}_{x_j,\,x_k}(x_j,x_k) \equiv &
 f_{S \cup \{j,\,k\}}(x_{S \cup \{j,\,k\}}) 
-f_{S \cup \{j\}}(x_{S \cup \{j\}}) \\
& -f_{S \cup \{k\}}(x_{S \cup \{k\}}) + f_{S}(x_{S}) 
\end{align*}
\item \blue{Kernel sampling can speed this up}
\red{but it is still slow}
\end{itemize}
\end{frame}
\end{comment}


\begin{frame}\frametitle{Marginal effect of age: 
computational efficiency measured \\
 by {\tt system.time()} in seconds} 
\begin{center}
\begin{tabular}{l|rr|rr}
& \multicolumn{4}{c}{\tt Computational Timings} \\ \hline
& \multicolumn{2}{c|}{\tt user} & \multicolumn{2}{c}{\tt elapsed} \\
Method & \multicolumn{1}{c}{s} & \multicolumn{1}{c|}{\%}  
& \multicolumn{1}{c}{s} & \multicolumn{1}{c}{\%} \\ \hline
Synthetic approximation   & 340 & 100 &  64 & 100 \\
Nearest neighbor &  32 &   9 &  20 &  31 \\
SRS: $K=30$          & 130 &  38 &  17 &  27 \\
SRS: $K=5$           & 22  &   6 &   3 &   5 \\  \hline
%SHAP: $t$, age-only   &  87 &     &  89 & \\    
%SHAP: $u$, sex-only   &  65 &     &  66 & \\    
%SHAP: $w$, weight-only& 120 &     & 120 & \\    
SV: $t$, age-only   &1610 &     &1610 & \\    
SV: $u$, sex-only   & 249 &     & 249 & \\    
SV: $w$, weight-only&2007 &     &2011 & \\    
SV: Synthetic approximation
%                      & 272 &  80 & 275 & 430   
                      &3866 &1137 &3870 &6047   
\end{tabular}
\end{center}
\end{frame}



\begin{frame}\frametitle{Marginal effects for dependent covariates and\\ 
computational efficiency }
\begin{itemize}
\item At first, it is quite surprising that FPD assumes independence since it has 
the term {\it dependence} in its name
\item Our novel marginals Nearest Neighbor and\\ 
Synthetic Approximation are computationally efficient
\item Yet the Shapley value marginals are very computationally demanding 
and often impractical
\item It is possible to exploit the structure of binary trees to 
  compute Shapley values by the so-called Tree SHAP algorithms\\
 Lundberg and Erion et al.\ 2020 {\it Nature Machine Intelligence}\\ 
\item For example, see the {\bf treeshap} R package for Random Forests\\
but this still might not me fast enough to be feasible
\item Furthermore, this has not been adapted to BART FWIW
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Marginal effects for dependent covariates and\\ 
computational efficiency }
\begin{itemize}
\item My {\bf BART3} package on {\tt github} has S3 methods for marginals
\end{itemize}
\begin{tabular}{lll} \hline
S3 & Assumes &  \\ 
method & $\perp$ & Marginal type \\ \hline
{\tt FPD} & Yes       & FPD \\
{\tt SRS} & Yes       & Simple random sampling \\
{\tt SHAP} & Yes      & SV \\
{\tt SHAP2} & Yes     & SV two-way interaction \\
{\tt NN}  & No        & Nearest neighbors \\
{\tt SHNN}& No        & SV with nearest neighbors \\
{\tt SHNN2}& No        & SV two-way interaction with nearest neighbors \\
\end{tabular}
%\begin{verbatim}
% {\tt 
% R> install.packages("Rcpp", dependencies=TRUE)\\
% R> install.packages("remotes", dependencies=TRUE)\\
% R> library("remotes")\\
% R> install\_github("rsparapa/bnptools/BART3")}
% %\end{verbatim}
\end{frame}


\begin{frame}\frametitle{Conclusion}
\begin{itemize}
\item This was an overview of BART and its place in machine learning
\item Our focus was on the BART prior for continuous outcomes
\item In particular, estimating marginal effects with BART
whether assuming independence or dependence
\item We contrasted Friedman's partial dependence
function with Shapley values
\item And we have described facilitating these
calculations with opportunities for bettering performance
statistically and computationally
\item We provide a reference implementation in the {\bf BART3} R package with {\it new and improved} marginal effects S3 functions
% the {\bf hbart} R package for heteroskedastic BART are on {\tt github}
\end{itemize}
 \end{frame}

\end{document}

{\tt 
R> install.packages("Rcpp", dependencies=TRUE)\\
R> install.packages("remotes", dependencies=TRUE)\\
R> library(remotes)\\
R> install\_github("rsparapa/bnptools/BART3")\\ 
R> install\_github("rsparapa/bnptools/hbart")
}
 \end{frame}

\end{document}
